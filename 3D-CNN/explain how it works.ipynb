{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "\n",
    "# Define the Flexible 3D CNN model class (no changes here)\n",
    "class Flexible3DCNN(nn.Module):\n",
    "    def __init__(self, conv_layers=3, filters=[8, 16, 32], pooling=\"avg\", activation=\"relu\", optimizer_type=\"adam\"):\n",
    "        super(Flexible3DCNN, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        self.pooling_type = pooling\n",
    "        self.activation_type = activation\n",
    "        self.optimizer_type = optimizer_type  # Store optimizer type as part of the model\n",
    "\n",
    "        # Initialize Convolutional Layers\n",
    "        in_channels = 3  # 3 channels for MRI data\n",
    "        for i in range(conv_layers):\n",
    "            self.layers.append(nn.Conv3d(in_channels, filters[i], kernel_size=3, padding=1))\n",
    "            in_channels = filters[i]\n",
    "\n",
    "        # Calculate flattened size dynamically after conv layers\n",
    "        sample_input = torch.zeros(1, 3, 48, 256, 256)  # Updated to match data dimensions\n",
    "        flattened_size = self.determine_flattened_size(sample_input)\n",
    "\n",
    "        # Fully connected classifier for 3 binary outputs (one per class)\n",
    "        self.fc = nn.Linear(flattened_size, 3)  # For 3 binary output classes\n",
    "\n",
    "    def determine_flattened_size(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = self.apply_activation(x)\n",
    "            if min(x.shape[2:]) >= 2:  # Apply pooling only if all spatial dimensions are >= 2\n",
    "                x = self.apply_pooling(x)\n",
    "        return x.view(-1).size(0)\n",
    "    \n",
    "    def apply_pooling(self, x):\n",
    "        return F.avg_pool3d(x, 2) if self.pooling_type == \"avg\" else F.max_pool3d(x, 2)\n",
    "    \n",
    "    def apply_activation(self, x):\n",
    "        return torch.sigmoid(x) if self.activation_type == \"sigmoid\" else F.relu(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            x = self.apply_activation(x)\n",
    "            if min(x.shape[2:]) >= 2:  # Apply pooling only if all spatial dimensions are >= 2\n",
    "                x = self.apply_pooling(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)  # Output logits for each class\n",
    "        return x\n",
    "\n",
    "# Define the loss function with BCEWithLogitsLoss\n",
    "def calculate_loss(preds, targets):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    return criterion(preds, targets.float())\n",
    "\n",
    "def compute_metrics(y_true, y_pred, y_proba):\n",
    "    labels = [\"abnormal\", \"acl\", \"meniscus\"]  # Label names for multi-label classification\n",
    "    y_true_np, y_pred_np, y_proba_np = np.array(y_true), np.array(y_pred), np.array(y_proba)\n",
    "    results = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        cm = confusion_matrix(y_true_np[:, i], y_pred_np[:, i], labels=[0, 1])\n",
    "        accuracy = accuracy_score(y_true_np[:, i], y_pred_np[:, i])\n",
    "        precision = precision_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
    "        recall = recall_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
    "        f1 = f1_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
    "        try:\n",
    "            log_loss_value = log_loss(y_true_np[:, i], y_proba_np[:, i], labels=[0, 1])\n",
    "        except ValueError as e:\n",
    "            log_loss_value = None\n",
    "        \n",
    "        # For display\n",
    "        print(f\"\\nMetrics for {label}:\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-Score: {f1}\")\n",
    "        print(f\"Log Loss: {log_loss_value}\")\n",
    "\n",
    "        results[label] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'log_loss': log_loss_value\n",
    "        }\n",
    "    return results\n",
    "\n",
    "\n",
    "def flatten_metrics(metrics, prefix):\n",
    "    flattened = {}\n",
    "    for label, label_metrics in metrics.items():\n",
    "        for metric_name, value in label_metrics.items():\n",
    "            flattened[f\"{prefix}_{label}_{metric_name}\"] = float(value) if isinstance(value, np.float64) else value\n",
    "    return flattened\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, valid_loader, epochs=5, learning_rate=0.001):\n",
    "    if model.optimizer_type.lower() == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif model.optimizer_type.lower() == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer type. Choose 'adam' or 'sgd'.\")\n",
    "\n",
    "    results = []  # Store summarized epoch metrics for table output\n",
    "    detailed_metrics = []  # Store metrics for `detailed_df` with all per-class data\n",
    "    final_y_true_valid, final_y_pred_proba_valid = None, None  # To store final epoch validation values\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        y_true_train, y_pred_train, y_pred_proba_train = [], [], []\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = calculate_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            y_true_train.extend(labels.numpy())\n",
    "            y_pred_train.extend((torch.sigmoid(outputs) > 0.5).detach().numpy())\n",
    "            y_pred_proba_train.extend(torch.sigmoid(outputs).detach().numpy())\n",
    "        \n",
    "        # Compute training metrics\n",
    "        train_metrics = compute_metrics(np.array(y_true_train), np.array(y_pred_train), np.array(y_pred_proba_train))\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        y_true_valid, y_pred_proba_valid = [], []\n",
    "        \n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = calculate_loss(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "                \n",
    "                y_true_valid.extend(labels.numpy())\n",
    "                y_pred_proba_valid.extend(torch.sigmoid(outputs).detach().numpy())\n",
    "        \n",
    "        y_pred_valid = (np.array(y_pred_proba_valid) > 0.5).astype(int)\n",
    "        valid_metrics = compute_metrics(np.array(y_true_valid), y_pred_valid, np.array(y_pred_proba_valid))\n",
    "\n",
    "        # Update final validation metrics for last epoch outputs\n",
    "        final_y_true_valid = np.array(y_true_valid)\n",
    "        final_y_pred_proba_valid = np.array(y_pred_proba_valid)\n",
    "\n",
    "        # Add detailed metrics (all classes, all epochs) for `detailed_df`\n",
    "        for phase, metrics in zip([\"Train\", \"Validation\"], [train_metrics, valid_metrics]):\n",
    "            for label, label_metrics in metrics.items():\n",
    "                detailed_metrics.append({\n",
    "                    \"Epoch\": epoch + 1,\n",
    "                    \"Phase\": phase,\n",
    "                    \"Class\": label,\n",
    "                    \"Accuracy\": label_metrics[\"accuracy\"],\n",
    "                    \"Precision\": label_metrics[\"precision\"],\n",
    "                    \"Recall\": label_metrics[\"recall\"],\n",
    "                    \"F1-Score\": label_metrics[\"f1\"],\n",
    "                    \"Log Loss\": label_metrics[\"log_loss\"]\n",
    "                })\n",
    "\n",
    "        # Aggregate metrics for `results` (summary output without per-class breakdown)\n",
    "        overall_train_metrics = flatten_metrics(train_metrics, \"Train\")\n",
    "        overall_valid_metrics = flatten_metrics(valid_metrics, \"Valid\")\n",
    "        \n",
    "        row_data = {\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Train Loss\": train_loss / len(train_loader),\n",
    "            \"Valid Loss\": valid_loss / len(valid_loader),\n",
    "        }\n",
    "        row_data.update(overall_train_metrics)\n",
    "        row_data.update(overall_valid_metrics)\n",
    "        results.append(row_data)\n",
    "\n",
    "    # Display summarized DataFrame (`summary_df`) with per-epoch metrics\n",
    "    summary_df = pd.DataFrame(results)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    display(summary_df.style.set_table_styles([{'selector': 'th', 'props': [('font-weight', 'bold')]}]))\n",
    "\n",
    "    # Create `detailed_df` with per-class metrics in columns, suitable for further analysis\n",
    "    detailed_df = pd.DataFrame(detailed_metrics)\n",
    "    return final_y_true_valid, final_y_pred_proba_valid\n",
    "\n",
    "# Convert labels from dictionaries to multi-label binary tensors\n",
    "def convert_labels_to_tensor(labels):\n",
    "    labels_tensor = torch.zeros(len(labels), 3)  # Three classes: abnormal, ACL, meniscus\n",
    "    for i, label_dict in enumerate(labels):\n",
    "        labels_tensor[i, 0] = label_dict['abnormal']\n",
    "        labels_tensor[i, 1] = label_dict['acl']\n",
    "        labels_tensor[i, 2] = label_dict['meniscus']\n",
    "    return labels_tensor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s a detailed explanation of how the code works when the model is configured with the following parameters:\n",
    "\n",
    "- **`conv_layers=3`**: Three convolutional layers.\n",
    "- **`filters=[8, 16, 32]`**: Number of filters in each convolutional layer.\n",
    "- **`pooling=\"avg\"`**: Average pooling is applied after convolution.\n",
    "- **`activation=\"relu\"`**: ReLU activation is used after convolution.\n",
    "- **`optimizer_type=\"adam\"`**: Adam optimizer is used for training.\n",
    "\n",
    "We’ll walk through the process from the perspective of a 3D image being inputted into the model and moving through its components.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Input (3D Image)**\n",
    "- **Input Shape**: A single input MRI scan is represented as a tensor of shape `(Batch Size, Channels, Depth, Height, Width)`.\n",
    "  - Example: For one image in the batch: `(1, 3, 48, 256, 256)`\n",
    "    - **Batch Size = 1**: One image in the batch.\n",
    "    - **Channels = 3**: RGB or similar multi-channel format.\n",
    "    - **Depth = 48**: Number of slices along the depth (z-axis).\n",
    "    - **Height = 256**: Vertical size of the image.\n",
    "    - **Width = 256**: Horizontal size of the image.\n",
    "\n",
    "The input represents a 3D volume with three channels.\n",
    "\n",
    "---\n",
    "\n",
    "### **2. First Convolutional Layer (Conv3D)**\n",
    "- **Operation**: A 3D convolution is applied.\n",
    "  - Input Shape: `(1, 3, 48, 256, 256)` (3 channels from the input).\n",
    "  - Filters: The first layer uses `8 filters`, so the output will have 8 channels.\n",
    "  - Kernel Size: `(3, 3, 3)` (default).\n",
    "  - Padding: Adds padding to preserve spatial dimensions.\n",
    "\n",
    "- **Output Shape**: After convolution:\n",
    "  - `(1, 8, 48, 256, 256)` (8 channels).\n",
    "\n",
    "- **Activation**: ReLU activation is applied to introduce non-linearity:\n",
    "  - Any negative values in the output are set to 0.\n",
    "\n",
    "- **Pooling**: Average pooling reduces the spatial dimensions by a factor of 2:\n",
    "  - `(1, 8, 24, 128, 128)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Second Convolutional Layer (Conv3D)**\n",
    "- **Operation**: A second 3D convolution is applied to the output of the first layer.\n",
    "  - Input Shape: `(1, 8, 24, 128, 128)` (8 channels from the first layer).\n",
    "  - Filters: The second layer uses `16 filters`.\n",
    "  - Kernel Size: `(3, 3, 3)`.\n",
    "  - Padding: Preserves spatial dimensions.\n",
    "\n",
    "- **Output Shape**: After convolution:\n",
    "  - `(1, 16, 24, 128, 128)`.\n",
    "\n",
    "- **Activation**: ReLU activation is applied again.\n",
    "\n",
    "- **Pooling**: Average pooling reduces spatial dimensions by a factor of 2:\n",
    "  - `(1, 16, 12, 64, 64)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Third Convolutional Layer (Conv3D)**\n",
    "- **Operation**: A third 3D convolution is applied.\n",
    "  - Input Shape: `(1, 16, 12, 64, 64)` (16 channels from the second layer).\n",
    "  - Filters: The third layer uses `32 filters`.\n",
    "  - Kernel Size: `(3, 3, 3)`.\n",
    "  - Padding: Preserves spatial dimensions.\n",
    "\n",
    "- **Output Shape**: After convolution:\n",
    "  - `(1, 32, 12, 64, 64)`.\n",
    "\n",
    "- **Activation**: ReLU activation is applied.\n",
    "\n",
    "- **Pooling**: Average pooling reduces spatial dimensions by a factor of 2:\n",
    "  - `(1, 32, 6, 32, 32)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Flattening**\n",
    "- **Operation**: The output from the final convolutional layer is flattened.\n",
    "  - Input Shape: `(1, 32, 6, 32, 32)` (32 channels, depth = 6, height = 32, width = 32).\n",
    "  - Flattened Size: Computed as \\( 32 \\times 6 \\times 32 \\times 32 = 196,608 \\).\n",
    "\n",
    "- **Output Shape**: The flattened tensor becomes a vector of size `(1, 196,608)`.\n",
    "\n",
    "---\n",
    "\n",
    "### **6. Fully Connected Layer**\n",
    "- **Operation**: The flattened vector is passed through a fully connected (dense) layer for classification.\n",
    "  - Input Shape: `(1, 196,608)`.\n",
    "  - Output Shape: `(1, 3)` (one logit for each class: `abnormal`, `acl`, `meniscus`).\n",
    "\n",
    "---\n",
    "\n",
    "### **7. Output**\n",
    "- **Output Logits**: The output is a raw score (logit) for each of the 3 classes. These logits represent unnormalized probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "### **8. Loss Computation**\n",
    "- **Loss Function**: Binary Cross-Entropy with Logits Loss (`BCEWithLogitsLoss`) is applied.\n",
    "  - Converts the logits to probabilities using the sigmoid function:\n",
    "    - \\( \\text{Sigmoid}(x) = \\frac{1}{1 + e^{-x}} \\).\n",
    "  - Compares the predicted probabilities to the ground truth multi-label targets.\n",
    "  - Example Target: `[1, 0, 1]` (indicating abnormalities and meniscus tear).\n",
    "\n",
    "---\n",
    "\n",
    "### **9. Optimizer (Adam)**\n",
    "- **Optimizer**: The Adam optimizer updates the model weights during backpropagation using the gradients computed from the loss.\n",
    "\n",
    "---\n",
    "\n",
    "### **10. Training Process**\n",
    "- **Steps**:\n",
    "  1. Input a batch of 3D images and labels.\n",
    "  2. Compute predictions by passing the input through the model.\n",
    "  3. Calculate loss between predictions and true labels.\n",
    "  4. Backpropagate to compute gradients.\n",
    "  5. Update model weights using the Adam optimizer.\n",
    "  6. Repeat for all batches in the training set.\n",
    "\n",
    "---\n",
    "\n",
    "### **11. Validation Process**\n",
    "- Similar to training but without updating weights (model evaluation mode).\n",
    "- Computes metrics like accuracy, precision, recall, F1 score, and log loss for each class.\n",
    "\n",
    "---\n",
    "\n",
    "### **Summary of the Workflow**\n",
    "1. **Input**: 3D MRI image of shape `(Batch Size, 3, Depth, Height, Width)`.\n",
    "2. **Convolution Layers**: Extract hierarchical features with increasing filter depth (8 → 16 → 32).\n",
    "3. **Pooling**: Reduces spatial dimensions while retaining important features.\n",
    "4. **Flattening**: Converts the 3D feature map into a 1D vector.\n",
    "5. **Fully Connected Layer**: Maps the flattened features to 3 binary outputs (classes).\n",
    "6. **Output**: Raw logits for each class, converted to probabilities for multi-label classification.\n",
    "7. **Training**: Minimizes the Binary Cross-Entropy loss using the Adam optimizer.\n",
    "\n",
    "This setup processes 3D medical images to classify them into three possible categories (`abnormal`, `acl`, `meniscus`) while allowing flexible configuration of layers and training parameters.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
