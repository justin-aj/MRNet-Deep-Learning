{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YVVjF_stwJ5D"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models.video import r3d_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "OuIPGV5gwN6F"
   },
   "outputs": [],
   "source": [
    "main_dir = \"../MRNet-v1.0\"\n",
    "train_path = os.path.join(main_dir, \"train\")\n",
    "valid_path = os.path.join(main_dir, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFz1mMT0wOkv",
    "outputId": "33f7868a-94d4-4ab9-ce81-ec4450ea23b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axial scan shape: (44, 256, 256)\n",
      "Coronal scan shape: (36, 256, 256)\n",
      "Sagittal scan shape: (36, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../MRNet-v1.0/train\"\n",
    "file_id = \"0000\"\n",
    "\n",
    "axial_path = f\"{base_dir}/axial/{file_id}.npy\"\n",
    "coronal_path = f\"{base_dir}/coronal/{file_id}.npy\"\n",
    "sagittal_path = f\"{base_dir}/sagittal/{file_id}.npy\"\n",
    "\n",
    "# Load MRI scans from the .npy files\n",
    "axial_scan = np.load(axial_path)\n",
    "coronal_scan = np.load(coronal_path)\n",
    "sagittal_scan = np.load(sagittal_path)\n",
    "\n",
    "# Check the shape of each MRI scan\n",
    "print(\"Axial scan shape:\", axial_scan.shape)\n",
    "print(\"Coronal scan shape:\", coronal_scan.shape)\n",
    "print(\"Sagittal scan shape:\", sagittal_scan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HHj1bFQZP3xj"
   },
   "outputs": [],
   "source": [
    "# Load labels from CSV files\n",
    "train_abnormal = pd.read_csv(os.path.join(main_dir, \"train-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "train_acl = pd.read_csv(os.path.join(main_dir, \"train-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "train_meniscus = pd.read_csv(os.path.join(main_dir, \"train-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "\n",
    "valid_abnormal = pd.read_csv(os.path.join(main_dir, \"valid-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "valid_acl = pd.read_csv(os.path.join(main_dir, \"valid-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "valid_meniscus = pd.read_csv(os.path.join(main_dir, \"valid-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "DJQbFwJCP416"
   },
   "outputs": [],
   "source": [
    "# Function to resize the depth of a scan to a target depth\n",
    "def resize_depth(scan, target_depth):\n",
    "    depth_factor = target_depth / scan.shape[0]\n",
    "    return zoom(scan, (depth_factor, 1, 1), order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Ktlwb5XCP77A"
   },
   "outputs": [],
   "source": [
    "# Function to pad a scan to a target shape\n",
    "def pad_to_shape(scan, target_shape):\n",
    "    padded_scan = np.zeros(target_shape, dtype=scan.dtype)\n",
    "    min_d, min_h, min_w = min(scan.shape[0], target_shape[0]), min(scan.shape[1], target_shape[1]), min(scan.shape[2], target_shape[2])\n",
    "    padded_scan[:min_d, :min_h, :min_w] = scan[:min_d, :min_h, :min_w]\n",
    "    return padded_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "-CEpNZSZhEcL"
   },
   "outputs": [],
   "source": [
    "# Function to load a specific range of MRI data with labels\n",
    "\n",
    "def load_mri_data(data_type=\"train\", start_idx=0, end_idx=9, target_shape=(48, 256, 256), target_depth=48):\n",
    "    \"\"\"\n",
    "    Loads MRI data from a specified range and resizes/pads each scan to a target shape.\n",
    "    Parameters:\n",
    "    - data_type: \"train\" or \"valid\"\n",
    "    - start_idx, end_idx: Range of file indices to load (e.g., 0 to 9 for train, 1130 to 1249 for valid)\n",
    "    - target_shape: Target shape for each scan after resizing and padding\n",
    "    - target_depth: Target depth for each scan to ensure consistent depth\n",
    "    \"\"\"\n",
    "    # Set data path and range\n",
    "    data_path = train_path if data_type == \"train\" else valid_path\n",
    "    axial_path, coronal_path, sagittal_path = Path(data_path) / \"axial\", Path(data_path) / \"coronal\", Path(data_path) / \"sagittal\"\n",
    "\n",
    "    # Select the appropriate labels dictionary based on data type\n",
    "    abnormal_labels = train_abnormal if data_type == \"train\" else valid_abnormal\n",
    "    acl_labels = train_acl if data_type == \"train\" else valid_acl\n",
    "    meniscus_labels = train_meniscus if data_type == \"train\" else valid_meniscus\n",
    "\n",
    "    # Initialize lists to store data and labels\n",
    "    mri_data, labels = [], []\n",
    "\n",
    "    # Load each MRI scan within the specified range\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        # Generate file name with zero-padded format (e.g., 0000, 0001, ...)\n",
    "        file_name = f\"{i:04}.npy\"\n",
    "\n",
    "        # Load and process each view with resizing and padding\n",
    "        axial_scan = pad_to_shape(resize_depth(np.load(axial_path / file_name), target_depth), target_shape)\n",
    "        coronal_scan = pad_to_shape(resize_depth(np.load(coronal_path / file_name), target_depth), target_shape)\n",
    "        sagittal_scan = pad_to_shape(resize_depth(np.load(sagittal_path / file_name), target_depth), target_shape)\n",
    "\n",
    "        # Combine the three views into one structure (3, depth, height, width)\n",
    "        combined_scan = np.stack([axial_scan, coronal_scan, sagittal_scan], axis=0)\n",
    "        mri_data.append(combined_scan)\n",
    "\n",
    "        # Retrieve actual labels for the current scan\n",
    "        abnormal_label = abnormal_labels.get(i, 0)  # Default to 0 if label is missing\n",
    "        acl_label = acl_labels.get(i, 0)\n",
    "        meniscus_label = meniscus_labels.get(i, 0)\n",
    "\n",
    "        # Append the actual labels\n",
    "        # print(\"Loaded\", i)\n",
    "        if (i%10==0):\n",
    "            print(\"Loaded, \", i) \n",
    "        labels.append({\"abnormal\": abnormal_label, \"acl\": acl_label, \"meniscus\": meniscus_label})\n",
    "\n",
    "    return np.array(mri_data), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_SSN7NPKqL5_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded,  0\n",
      "Loaded,  10\n",
      "Train data shape: (20, 3, 48, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from indices 1130 to 1249\n",
    "train_data, train_labels = load_mri_data(data_type=\"train\", start_idx=0, end_idx=19)\n",
    "\n",
    "# Check data shapes and labels\n",
    "print(\"Train data shape:\", train_data.shape)  # Expected: (120, 3, 48, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "T25feCXJps1p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded,  1130\n",
      "Loaded,  1140\n",
      "Validation data shape: (20, 3, 48, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from indices 1130 to 1249\n",
    "valid_data, valid_labels = load_mri_data(data_type=\"valid\", start_idx=1130, end_idx=1149)\n",
    "\n",
    "# Check data shapes and labels\n",
    "print(\"Validation data shape:\", valid_data.shape)  # Expected: (120, 3, 48, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "g5qNv2c7QEOT"
   },
   "outputs": [],
   "source": [
    "# Set display options to avoid truncation\n",
    "pd.set_option('display.max_rows', None)      # Show all rows in the DataFrame\n",
    "pd.set_option('display.max_columns', None)   # Show all columns in the DataFrame\n",
    "pd.set_option('display.width', None)         # Expand display width to accommodate more columns\n",
    "pd.set_option('display.max_colwidth', None)  # Expand column width if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hmSgr-DtSrwk"
   },
   "outputs": [],
   "source": [
    "# Define the Flexible 3D ResNet model class\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, pretrained=True, num_classes=3, optimizer_type=\"adam\"):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.optimizer_type = optimizer_type  # Store optimizer type as part of the model\n",
    "\n",
    "        # Load a pre-trained 3D ResNet-18 model\n",
    "        self.resnet3d = r3d_18(pretrained=pretrained)\n",
    "\n",
    "        # Replace the final fully connected layer to match the desired output size\n",
    "        in_features = self.resnet3d.fc.in_features\n",
    "        self.resnet3d.fc = nn.Linear(in_features, num_classes)  # 3 binary outputs (one per class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet3d(x)\n",
    "\n",
    "# Define the loss function with BCEWithLogitsLoss\n",
    "def calculate_loss(preds, targets):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    return criterion(preds, targets.float())\n",
    "\n",
    "# Compute metrics for multi-label classification\n",
    "def compute_metrics(y_true, y_pred, y_proba):\n",
    "    labels = [\"abnormal\", \"acl\", \"meniscus\"]  # Label names for multi-label classification\n",
    "    y_true_np, y_pred_np, y_proba_np = np.array(y_true), np.array(y_pred), np.array(y_proba)\n",
    "    results = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        cm = confusion_matrix(y_true_np[:, i], y_pred_np[:, i], labels=[0, 1])\n",
    "        accuracy = accuracy_score(y_true_np[:, i], y_pred_np[:, i])\n",
    "        precision = precision_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
    "        recall = recall_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
    "        f1 = f1_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
    "        try:\n",
    "            log_loss_value = log_loss(y_true_np[:, i], y_proba_np[:, i], labels=[0, 1])\n",
    "        except ValueError:\n",
    "            log_loss_value = None\n",
    "\n",
    "        print(f\"\\nMetrics for {label}:\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-Score: {f1}\")\n",
    "        print(f\"Log Loss: {log_loss_value}\")\n",
    "\n",
    "        results[label] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'log_loss': log_loss_value\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Flatten metrics for logging\n",
    "def flatten_metrics(metrics, prefix):\n",
    "    flattened = {}\n",
    "    for label, label_metrics in metrics.items():\n",
    "        for metric_name, value in label_metrics.items():\n",
    "            flattened[f\"{prefix}_{label}_{metric_name}\"] = float(value) if isinstance(value, np.float64) else value\n",
    "    return flattened\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model, train_loader, valid_loader, epochs=5, learning_rate=0.001):\n",
    "    if model.optimizer_type.lower() == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif model.optimizer_type.lower() == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer type. Choose 'adam' or 'sgd'.\")\n",
    "\n",
    "    results = []  # Store summarized epoch metrics for table output\n",
    "    detailed_metrics = []  # Store metrics for `detailed_df` with all per-class data\n",
    "    final_y_true_valid, final_y_pred_proba_valid = None, None  # To store final epoch validation values\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        y_true_train, y_pred_train, y_pred_proba_train = [], [], []\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = calculate_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            y_true_train.extend(labels.numpy())\n",
    "            y_pred_train.extend((torch.sigmoid(outputs) > 0.5).detach().numpy())\n",
    "            y_pred_proba_train.extend(torch.sigmoid(outputs).detach().numpy())\n",
    "\n",
    "        # Compute training metrics\n",
    "        train_metrics = compute_metrics(np.array(y_true_train), np.array(y_pred_train), np.array(y_pred_proba_train))\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        y_true_valid, y_pred_proba_valid = [], []\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = calculate_loss(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                y_true_valid.extend(labels.numpy())\n",
    "                y_pred_proba_valid.extend(torch.sigmoid(outputs).detach().numpy())\n",
    "\n",
    "        y_pred_valid = (np.array(y_pred_proba_valid) > 0.5).astype(int)\n",
    "        valid_metrics = compute_metrics(np.array(y_true_valid), y_pred_valid, np.array(y_pred_proba_valid))\n",
    "\n",
    "        # Update final validation metrics for last epoch outputs\n",
    "        final_y_true_valid = np.array(y_true_valid)\n",
    "        final_y_pred_proba_valid = np.array(y_pred_proba_valid)\n",
    "\n",
    "        # Add detailed metrics (all classes, all epochs) for `detailed_df`\n",
    "        for phase, metrics in zip([\"Train\", \"Validation\"], [train_metrics, valid_metrics]):\n",
    "            for label, label_metrics in metrics.items():\n",
    "                detailed_metrics.append({\n",
    "                    \"Epoch\": epoch + 1,\n",
    "                    \"Phase\": phase,\n",
    "                    \"Class\": label,\n",
    "                    \"Accuracy\": label_metrics[\"accuracy\"],\n",
    "                    \"Precision\": label_metrics[\"precision\"],\n",
    "                    \"Recall\": label_metrics[\"recall\"],\n",
    "                    \"F1-Score\": label_metrics[\"f1\"],\n",
    "                    \"Log Loss\": label_metrics[\"log_loss\"]\n",
    "                })\n",
    "\n",
    "        # Aggregate metrics for `results` (summary output without per-class breakdown)\n",
    "        overall_train_metrics = flatten_metrics(train_metrics, \"Train\")\n",
    "        overall_valid_metrics = flatten_metrics(valid_metrics, \"Valid\")\n",
    "\n",
    "        row_data = {\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Train Loss\": train_loss / len(train_loader),\n",
    "            \"Valid Loss\": valid_loss / len(valid_loader),\n",
    "        }\n",
    "        #format and print the row data\n",
    "\n",
    "        print(row_data)\n",
    "        row_data.update(overall_train_metrics)\n",
    "        row_data.update(overall_valid_metrics)\n",
    "        results.append(row_data)\n",
    "\n",
    "    # Display summarized DataFrame (`summary_df`) with per-epoch metrics\n",
    "    summary_df = pd.DataFrame(results)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    display(summary_df.style.set_table_styles([{'selector': 'th', 'props': [('font-weight', 'bold')]}]))\n",
    "\n",
    "    # Create `detailed_df` with per-class metrics in columns, suitable for further analysis\n",
    "    detailed_df = pd.DataFrame(detailed_metrics)\n",
    "    return final_y_true_valid, final_y_pred_proba_valid\n",
    "\n",
    "# Convert labels from dictionaries to multi-label binary tensors\n",
    "def convert_labels_to_tensor(labels):\n",
    "    labels_tensor = torch.zeros(len(labels), 3)  # Three classes: abnormal, ACL, meniscus\n",
    "    for i, label_dict in enumerate(labels):\n",
    "        labels_tensor[i, 0] = label_dict['abnormal']\n",
    "        labels_tensor[i, 1] = label_dict['acl']\n",
    "        labels_tensor[i, 2] = label_dict['meniscus']\n",
    "    return labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "XbmQYxR5QR5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader - Batch Size: 8, Num Workers: 4\n",
      "Validation DataLoader - Batch Size: 8, Num Workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Function to convert a list of dictionaries to a tensor\n",
    "def convert_labels_to_tensor(label_dicts):\n",
    "    # Convert each dictionary's values into a list and create a tensor from it\n",
    "    labels_as_lists = [list(label.values()) for label in label_dicts]\n",
    "    return torch.tensor(labels_as_lists, dtype=torch.float32)\n",
    "\n",
    "# Apply this to train and validation labels\n",
    "train_labels_tensor = convert_labels_to_tensor(train_labels)\n",
    "valid_labels_tensor = convert_labels_to_tensor(valid_labels)\n",
    "\n",
    "# Create TensorDatasets for training and validation data\n",
    "train_tensor = TensorDataset(torch.tensor(train_data).float(), train_labels_tensor)\n",
    "valid_tensor = TensorDataset(torch.tensor(valid_data).float(), valid_labels_tensor)\n",
    "\n",
    "# Adjust batch size to avoid high memory usage\n",
    "batch_size = 8  # Adjust based on available memory\n",
    "\n",
    "# Create DataLoaders with updated parameters\n",
    "train_loader = DataLoader(\n",
    "    train_tensor,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4  # Adjust based on your CPU cores and memory\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_tensor,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Print out DataLoader settings for confirmation\n",
    "print(f\"Train DataLoader - Batch Size: {batch_size}, Num Workers: 4\")\n",
    "print(f\"Validation DataLoader - Batch Size: {batch_size}, Num Workers: 4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vgKp7wZlQTxK"
   },
   "outputs": [],
   "source": [
    "def plot_roc_and_calculate_auc(y_true, y_pred_proba, class_names=[\"abnormal\", \"acl\", \"meniscus\"]):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    auc_scores = {}\n",
    "\n",
    "    for i in range(y_true.shape[1]):  # Assuming y_true and y_pred_proba are (num_samples, num_classes)\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores[class_names[i]] = roc_auc  # Store AUC for each class\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "    # Plot the random classifier line\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve for Each Class\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print AUC scores for each class\n",
    "    print(\"AUC Scores for each class:\")\n",
    "    for class_name, auc_score in auc_scores.items():\n",
    "        print(f\"{class_name}: {auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "srhCDbAYQWCs"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yashi\\anaconda3\\envs\\sml\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\yashi\\anaconda3\\envs\\sml\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1610612736 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet3D(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, optimizer_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Run the training and evaluation function and capture the outputs\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m y_true_valid_np, y_pred_proba_valid_np \u001b[38;5;241m=\u001b[39m train_and_evaluate(model, train_loader, valid_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true_valid_np shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_true_valid_np\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred_proba_valid_np shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_pred_proba_valid_np\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[32], line 85\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_loader, valid_loader, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     84\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 85\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     86\u001b[0m     loss \u001b[38;5;241m=\u001b[39m calculate_loss(outputs, labels)\n\u001b[0;32m     87\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[32], line 15\u001b[0m, in \u001b[0;36mResNet3D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet3d(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torchvision\\models\\video\\resnet.py:251\u001b[0m, in \u001b[0;36mVideoResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[0;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    254\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:725\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:720\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    710\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    711\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    719\u001b[0m     )\n\u001b[1;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    722\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1610612736 bytes."
     ]
    }
   ],
   "source": [
    "model = ResNet3D(pretrained=True, num_classes=3, optimizer_type=\"adam\")\n",
    "\n",
    "# Run the training and evaluation function and capture the outputs\n",
    "y_true_valid_np, y_pred_proba_valid_np = train_and_evaluate(model, train_loader, valid_loader, epochs=1, learning_rate=0.1)\n",
    "\n",
    "print(\"y_true_valid_np shape:\", y_true_valid_np.shape)\n",
    "print(\"y_pred_proba_valid_np shape:\", y_pred_proba_valid_np.shape)\n",
    "\n",
    "# Directly call the function with `y_true_valid_np` and `y_pred_proba_valid_np`\n",
    "plot_roc_and_calculate_auc(y_true_valid_np, y_pred_proba_valid_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
