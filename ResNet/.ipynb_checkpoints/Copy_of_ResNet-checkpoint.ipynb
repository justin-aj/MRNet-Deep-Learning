{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YVVjF_stwJ5D"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from scipy.ndimage import zoom\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import models, transforms\n",
    "from pathlib import Path\n",
    "\n",
    "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import warnings\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision.models.video import r3d_18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OuIPGV5gwN6F"
   },
   "outputs": [],
   "source": [
    "main_dir = \"../MRNet-v1.0\"\n",
    "train_path = os.path.join(main_dir, \"train\")\n",
    "valid_path = os.path.join(main_dir, \"valid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFz1mMT0wOkv",
    "outputId": "33f7868a-94d4-4ab9-ce81-ec4450ea23b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Axial scan shape: (44, 256, 256)\n",
      "Coronal scan shape: (36, 256, 256)\n",
      "Sagittal scan shape: (36, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "base_dir = \"../MRNet-v1.0/train\"\n",
    "file_id = \"0000\"\n",
    "\n",
    "axial_path = f\"{base_dir}/axial/{file_id}.npy\"\n",
    "coronal_path = f\"{base_dir}/coronal/{file_id}.npy\"\n",
    "sagittal_path = f\"{base_dir}/sagittal/{file_id}.npy\"\n",
    "\n",
    "# Load MRI scans from the .npy files\n",
    "axial_scan = np.load(axial_path)\n",
    "coronal_scan = np.load(coronal_path)\n",
    "sagittal_scan = np.load(sagittal_path)\n",
    "\n",
    "# Check the shape of each MRI scan\n",
    "print(\"Axial scan shape:\", axial_scan.shape)\n",
    "print(\"Coronal scan shape:\", coronal_scan.shape)\n",
    "print(\"Sagittal scan shape:\", sagittal_scan.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HHj1bFQZP3xj"
   },
   "outputs": [],
   "source": [
    "# Load labels from CSV files\n",
    "train_abnormal = pd.read_csv(os.path.join(main_dir, \"train-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "train_acl = pd.read_csv(os.path.join(main_dir, \"train-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "train_meniscus = pd.read_csv(os.path.join(main_dir, \"train-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "\n",
    "valid_abnormal = pd.read_csv(os.path.join(main_dir, \"valid-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "valid_acl = pd.read_csv(os.path.join(main_dir, \"valid-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
    "valid_meniscus = pd.read_csv(os.path.join(main_dir, \"valid-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "DJQbFwJCP416"
   },
   "outputs": [],
   "source": [
    "# Function to resize the depth of a scan to a target depth\n",
    "def resize_depth(scan, target_depth):\n",
    "    depth_factor = target_depth / scan.shape[0]\n",
    "    return zoom(scan, (depth_factor, 1, 1), order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Ktlwb5XCP77A"
   },
   "outputs": [],
   "source": [
    "# Function to pad a scan to a target shape\n",
    "def pad_to_shape(scan, target_shape):\n",
    "    padded_scan = np.zeros(target_shape, dtype=scan.dtype)\n",
    "    min_d, min_h, min_w = min(scan.shape[0], target_shape[0]), min(scan.shape[1], target_shape[1]), min(scan.shape[2], target_shape[2])\n",
    "    padded_scan[:min_d, :min_h, :min_w] = scan[:min_d, :min_h, :min_w]\n",
    "    return padded_scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-CEpNZSZhEcL"
   },
   "outputs": [],
   "source": [
    "# Function to load a specific range of MRI data with labels\n",
    "\n",
    "def load_mri_data(data_type=\"train\", start_idx=0, end_idx=9, target_shape=(48, 256, 256), target_depth=48):\n",
    "    \"\"\"\n",
    "    Loads MRI data from a specified range and resizes/pads each scan to a target shape.\n",
    "    Parameters:\n",
    "    - data_type: \"train\" or \"valid\"\n",
    "    - start_idx, end_idx: Range of file indices to load (e.g., 0 to 9 for train, 1130 to 1249 for valid)\n",
    "    - target_shape: Target shape for each scan after resizing and padding\n",
    "    - target_depth: Target depth for each scan to ensure consistent depth\n",
    "    \"\"\"\n",
    "    # Set data path and range\n",
    "    data_path = train_path if data_type == \"train\" else valid_path\n",
    "    axial_path, coronal_path, sagittal_path = Path(data_path) / \"axial\", Path(data_path) / \"coronal\", Path(data_path) / \"sagittal\"\n",
    "\n",
    "    # Select the appropriate labels dictionary based on data type\n",
    "    abnormal_labels = train_abnormal if data_type == \"train\" else valid_abnormal\n",
    "    acl_labels = train_acl if data_type == \"train\" else valid_acl\n",
    "    meniscus_labels = train_meniscus if data_type == \"train\" else valid_meniscus\n",
    "\n",
    "    # Initialize lists to store data and labels\n",
    "    mri_data, labels = [], []\n",
    "\n",
    "    # Load each MRI scan within the specified range\n",
    "    for i in range(start_idx, end_idx + 1):\n",
    "        # Generate file name with zero-padded format (e.g., 0000, 0001, ...)\n",
    "        file_name = f\"{i:04}.npy\"\n",
    "\n",
    "        # Load and process each view with resizing and padding\n",
    "        axial_scan = pad_to_shape(resize_depth(np.load(axial_path / file_name), target_depth), target_shape)\n",
    "        coronal_scan = pad_to_shape(resize_depth(np.load(coronal_path / file_name), target_depth), target_shape)\n",
    "        sagittal_scan = pad_to_shape(resize_depth(np.load(sagittal_path / file_name), target_depth), target_shape)\n",
    "\n",
    "        # Combine the three views into one structure (3, depth, height, width)\n",
    "        combined_scan = np.stack([axial_scan, coronal_scan, sagittal_scan], axis=0)\n",
    "        mri_data.append(combined_scan)\n",
    "\n",
    "        # Retrieve actual labels for the current scan\n",
    "        abnormal_label = abnormal_labels.get(i, 0)  # Default to 0 if label is missing\n",
    "        acl_label = acl_labels.get(i, 0)\n",
    "        meniscus_label = meniscus_labels.get(i, 0)\n",
    "\n",
    "        # Append the actual labels\n",
    "        # print(\"Loaded\", i)\n",
    "        if (i%10==0):\n",
    "            print(\"Loaded, \", i) \n",
    "        labels.append({\"abnormal\": abnormal_label, \"acl\": acl_label, \"meniscus\": meniscus_label})\n",
    "\n",
    "    return np.array(mri_data), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "_SSN7NPKqL5_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0\n",
      "Loaded 1\n",
      "Loaded 2\n",
      "Loaded 3\n",
      "Loaded 4\n",
      "Loaded 5\n",
      "Loaded 6\n",
      "Loaded 7\n",
      "Loaded 8\n",
      "Loaded 9\n",
      "Loaded 10\n",
      "Loaded 11\n",
      "Loaded 12\n",
      "Loaded 13\n",
      "Loaded 14\n",
      "Loaded 15\n",
      "Loaded 16\n",
      "Loaded 17\n",
      "Loaded 18\n",
      "Loaded 19\n",
      "Loaded 20\n",
      "Loaded 21\n",
      "Loaded 22\n",
      "Loaded 23\n",
      "Loaded 24\n",
      "Loaded 25\n",
      "Loaded 26\n",
      "Loaded 27\n",
      "Loaded 28\n",
      "Loaded 29\n",
      "Loaded 30\n",
      "Loaded 31\n",
      "Loaded 32\n",
      "Loaded 33\n",
      "Loaded 34\n",
      "Loaded 35\n",
      "Loaded 36\n",
      "Loaded 37\n",
      "Loaded 38\n",
      "Loaded 39\n",
      "Loaded 40\n",
      "Loaded 41\n",
      "Loaded 42\n",
      "Loaded 43\n",
      "Loaded 44\n",
      "Loaded 45\n",
      "Loaded 46\n",
      "Loaded 47\n",
      "Loaded 48\n",
      "Loaded 49\n",
      "Loaded 50\n",
      "Loaded 51\n",
      "Loaded 52\n",
      "Loaded 53\n",
      "Loaded 54\n",
      "Loaded 55\n",
      "Loaded 56\n",
      "Loaded 57\n",
      "Loaded 58\n",
      "Loaded 59\n",
      "Loaded 60\n",
      "Loaded 61\n",
      "Loaded 62\n",
      "Loaded 63\n",
      "Loaded 64\n",
      "Loaded 65\n",
      "Loaded 66\n",
      "Loaded 67\n",
      "Loaded 68\n",
      "Loaded 69\n",
      "Loaded 70\n",
      "Loaded 71\n",
      "Loaded 72\n",
      "Loaded 73\n",
      "Loaded 74\n",
      "Loaded 75\n",
      "Loaded 76\n",
      "Loaded 77\n",
      "Loaded 78\n",
      "Loaded 79\n",
      "Loaded 80\n",
      "Loaded 81\n",
      "Loaded 82\n",
      "Loaded 83\n",
      "Loaded 84\n",
      "Loaded 85\n",
      "Loaded 86\n",
      "Loaded 87\n",
      "Loaded 88\n",
      "Loaded 89\n",
      "Loaded 90\n",
      "Loaded 91\n",
      "Loaded 92\n",
      "Loaded 93\n",
      "Loaded 94\n",
      "Loaded 95\n",
      "Loaded 96\n",
      "Loaded 97\n",
      "Loaded 98\n",
      "Loaded 99\n",
      "Loaded 100\n",
      "Loaded 101\n",
      "Loaded 102\n",
      "Loaded 103\n",
      "Loaded 104\n",
      "Loaded 105\n",
      "Loaded 106\n",
      "Loaded 107\n",
      "Loaded 108\n",
      "Loaded 109\n",
      "Loaded 110\n",
      "Loaded 111\n",
      "Loaded 112\n",
      "Loaded 113\n",
      "Loaded 114\n",
      "Loaded 115\n",
      "Loaded 116\n",
      "Loaded 117\n",
      "Loaded 118\n",
      "Loaded 119\n",
      "Train data shape: (120, 3, 48, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from indices 1130 to 1249\n",
    "train_data, train_labels = load_mri_data(data_type=\"train\", start_idx=0, end_idx=19)\n",
    "\n",
    "# Check data shapes and labels\n",
    "print(\"Train data shape:\", train_data.shape)  # Expected: (120, 3, 48, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "T25feCXJps1p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1130\n",
      "Loaded 1131\n",
      "Loaded 1132\n",
      "Loaded 1133\n",
      "Loaded 1134\n",
      "Loaded 1135\n",
      "Loaded 1136\n",
      "Loaded 1137\n",
      "Loaded 1138\n",
      "Loaded 1139\n",
      "Loaded 1140\n",
      "Loaded 1141\n",
      "Loaded 1142\n",
      "Loaded 1143\n",
      "Loaded 1144\n",
      "Loaded 1145\n",
      "Loaded 1146\n",
      "Loaded 1147\n",
      "Loaded 1148\n",
      "Loaded 1149\n",
      "Loaded 1150\n",
      "Loaded 1151\n",
      "Loaded 1152\n",
      "Loaded 1153\n",
      "Loaded 1154\n",
      "Loaded 1155\n",
      "Loaded 1156\n",
      "Loaded 1157\n",
      "Loaded 1158\n",
      "Loaded 1159\n",
      "Loaded 1160\n",
      "Loaded 1161\n",
      "Loaded 1162\n",
      "Loaded 1163\n",
      "Loaded 1164\n",
      "Loaded 1165\n",
      "Loaded 1166\n",
      "Loaded 1167\n",
      "Loaded 1168\n",
      "Loaded 1169\n",
      "Loaded 1170\n",
      "Loaded 1171\n",
      "Loaded 1172\n",
      "Loaded 1173\n",
      "Loaded 1174\n",
      "Loaded 1175\n",
      "Loaded 1176\n",
      "Loaded 1177\n",
      "Loaded 1178\n",
      "Loaded 1179\n",
      "Loaded 1180\n",
      "Loaded 1181\n",
      "Loaded 1182\n",
      "Loaded 1183\n",
      "Loaded 1184\n",
      "Loaded 1185\n",
      "Loaded 1186\n",
      "Loaded 1187\n",
      "Loaded 1188\n",
      "Loaded 1189\n",
      "Loaded 1190\n",
      "Loaded 1191\n",
      "Loaded 1192\n",
      "Loaded 1193\n",
      "Loaded 1194\n",
      "Loaded 1195\n",
      "Loaded 1196\n",
      "Loaded 1197\n",
      "Loaded 1198\n",
      "Loaded 1199\n",
      "Loaded 1200\n",
      "Loaded 1201\n",
      "Loaded 1202\n",
      "Loaded 1203\n",
      "Loaded 1204\n",
      "Loaded 1205\n",
      "Loaded 1206\n",
      "Loaded 1207\n",
      "Loaded 1208\n",
      "Loaded 1209\n",
      "Loaded 1210\n",
      "Loaded 1211\n",
      "Loaded 1212\n",
      "Loaded 1213\n",
      "Loaded 1214\n",
      "Loaded 1215\n",
      "Loaded 1216\n",
      "Loaded 1217\n",
      "Loaded 1218\n",
      "Loaded 1219\n",
      "Loaded 1220\n",
      "Loaded 1221\n",
      "Loaded 1222\n",
      "Loaded 1223\n",
      "Loaded 1224\n",
      "Loaded 1225\n",
      "Loaded 1226\n",
      "Loaded 1227\n",
      "Loaded 1228\n",
      "Loaded 1229\n",
      "Loaded 1230\n",
      "Loaded 1231\n",
      "Loaded 1232\n",
      "Loaded 1233\n",
      "Loaded 1234\n",
      "Loaded 1235\n",
      "Loaded 1236\n",
      "Loaded 1237\n",
      "Loaded 1238\n",
      "Loaded 1239\n",
      "Loaded 1240\n",
      "Loaded 1241\n",
      "Loaded 1242\n",
      "Loaded 1243\n",
      "Loaded 1244\n",
      "Loaded 1245\n",
      "Loaded 1246\n",
      "Loaded 1247\n",
      "Loaded 1248\n",
      "Loaded 1249\n",
      "Validation data shape: (120, 3, 48, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "# Load the validation data from indices 1130 to 1249\n",
    "valid_data, valid_labels = load_mri_data(data_type=\"valid\", start_idx=1130, end_idx=1149)\n",
    "\n",
    "# Check data shapes and labels\n",
    "print(\"Validation data shape:\", valid_data.shape)  # Expected: (120, 3, 48, 256, 256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "g5qNv2c7QEOT"
   },
   "outputs": [],
   "source": [
    "# Set display options to avoid truncation\n",
    "pd.set_option('display.max_rows', None)      # Show all rows in the DataFrame\n",
    "pd.set_option('display.max_columns', None)   # Show all columns in the DataFrame\n",
    "pd.set_option('display.width', None)         # Expand display width to accommodate more columns\n",
    "pd.set_option('display.max_colwidth', None)  # Expand column width if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "hmSgr-DtSrwk"
   },
   "outputs": [],
   "source": [
    "# Define the Flexible 3D ResNet model class\n",
    "class ResNet3D(nn.Module):\n",
    "    def __init__(self, pretrained=True, num_classes=3, optimizer_type=\"adam\"):\n",
    "        super(ResNet3D, self).__init__()\n",
    "        self.optimizer_type = optimizer_type  # Store optimizer type as part of the model\n",
    "\n",
    "        # Load a pre-trained 3D ResNet-18 model\n",
    "        self.resnet3d = r3d_18(pretrained=pretrained)\n",
    "\n",
    "        # Replace the final fully connected layer to match the desired output size\n",
    "        in_features = self.resnet3d.fc.in_features\n",
    "        self.resnet3d.fc = nn.Linear(in_features, num_classes)  # 3 binary outputs (one per class)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet3d(x)\n",
    "\n",
    "# Define the loss function with BCEWithLogitsLoss\n",
    "def calculate_loss(preds, targets):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    return criterion(preds, targets.float())\n",
    "\n",
    "# Compute metrics for multi-label classification\n",
    "def compute_metrics(y_true, y_pred, y_proba):\n",
    "    labels = [\"abnormal\", \"acl\", \"meniscus\"]  # Label names for multi-label classification\n",
    "    y_true_np, y_pred_np, y_proba_np = np.array(y_true), np.array(y_pred), np.array(y_proba)\n",
    "    results = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        cm = confusion_matrix(y_true_np[:, i], y_pred_np[:, i], labels=[0, 1])\n",
    "        accuracy = accuracy_score(y_true_np[:, i], y_pred_np[:, i])\n",
    "        precision = precision_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
    "        recall = recall_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
    "        f1 = f1_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
    "        try:\n",
    "            log_loss_value = log_loss(y_true_np[:, i], y_proba_np[:, i], labels=[0, 1])\n",
    "        except ValueError:\n",
    "            log_loss_value = None\n",
    "\n",
    "        print(f\"\\nMetrics for {label}:\")\n",
    "        print(f\"Confusion Matrix:\\n{cm}\")\n",
    "        print(f\"Accuracy: {accuracy}\")\n",
    "        print(f\"Precision: {precision}\")\n",
    "        print(f\"Recall: {recall}\")\n",
    "        print(f\"F1-Score: {f1}\")\n",
    "        print(f\"Log Loss: {log_loss_value}\")\n",
    "\n",
    "        results[label] = {\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'log_loss': log_loss_value\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# Flatten metrics for logging\n",
    "def flatten_metrics(metrics, prefix):\n",
    "    flattened = {}\n",
    "    for label, label_metrics in metrics.items():\n",
    "        for metric_name, value in label_metrics.items():\n",
    "            flattened[f\"{prefix}_{label}_{metric_name}\"] = float(value) if isinstance(value, np.float64) else value\n",
    "    return flattened\n",
    "\n",
    "# Training and evaluation function\n",
    "def train_and_evaluate(model, train_loader, valid_loader, epochs=5, learning_rate=0.001):\n",
    "    if model.optimizer_type.lower() == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif model.optimizer_type.lower() == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid optimizer type. Choose 'adam' or 'sgd'.\")\n",
    "\n",
    "    results = []  # Store summarized epoch metrics for table output\n",
    "    detailed_metrics = []  # Store metrics for `detailed_df` with all per-class data\n",
    "    final_y_true_valid, final_y_pred_proba_valid = None, None  # To store final epoch validation values\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        y_true_train, y_pred_train, y_pred_proba_train = [], [], []\n",
    "\n",
    "        # Training loop\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = calculate_loss(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            y_true_train.extend(labels.numpy())\n",
    "            y_pred_train.extend((torch.sigmoid(outputs) > 0.5).detach().numpy())\n",
    "            y_pred_proba_train.extend(torch.sigmoid(outputs).detach().numpy())\n",
    "\n",
    "        # Compute training metrics\n",
    "        train_metrics = compute_metrics(np.array(y_true_train), np.array(y_pred_train), np.array(y_pred_proba_train))\n",
    "\n",
    "        model.eval()\n",
    "        valid_loss = 0\n",
    "        y_true_valid, y_pred_proba_valid = [], []\n",
    "\n",
    "        # Validation loop\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = calculate_loss(outputs, labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                y_true_valid.extend(labels.numpy())\n",
    "                y_pred_proba_valid.extend(torch.sigmoid(outputs).detach().numpy())\n",
    "\n",
    "        y_pred_valid = (np.array(y_pred_proba_valid) > 0.5).astype(int)\n",
    "        valid_metrics = compute_metrics(np.array(y_true_valid), y_pred_valid, np.array(y_pred_proba_valid))\n",
    "\n",
    "        # Update final validation metrics for last epoch outputs\n",
    "        final_y_true_valid = np.array(y_true_valid)\n",
    "        final_y_pred_proba_valid = np.array(y_pred_proba_valid)\n",
    "\n",
    "        # Add detailed metrics (all classes, all epochs) for `detailed_df`\n",
    "        for phase, metrics in zip([\"Train\", \"Validation\"], [train_metrics, valid_metrics]):\n",
    "            for label, label_metrics in metrics.items():\n",
    "                detailed_metrics.append({\n",
    "                    \"Epoch\": epoch + 1,\n",
    "                    \"Phase\": phase,\n",
    "                    \"Class\": label,\n",
    "                    \"Accuracy\": label_metrics[\"accuracy\"],\n",
    "                    \"Precision\": label_metrics[\"precision\"],\n",
    "                    \"Recall\": label_metrics[\"recall\"],\n",
    "                    \"F1-Score\": label_metrics[\"f1\"],\n",
    "                    \"Log Loss\": label_metrics[\"log_loss\"]\n",
    "                })\n",
    "\n",
    "        # Aggregate metrics for `results` (summary output without per-class breakdown)\n",
    "        overall_train_metrics = flatten_metrics(train_metrics, \"Train\")\n",
    "        overall_valid_metrics = flatten_metrics(valid_metrics, \"Valid\")\n",
    "\n",
    "        row_data = {\n",
    "            \"Epoch\": epoch + 1,\n",
    "            \"Train Loss\": train_loss / len(train_loader),\n",
    "            \"Valid Loss\": valid_loss / len(valid_loader),\n",
    "        }\n",
    "        #format and print the row data\n",
    "\n",
    "        print(row_data)\n",
    "        row_data.update(overall_train_metrics)\n",
    "        row_data.update(overall_valid_metrics)\n",
    "        results.append(row_data)\n",
    "\n",
    "    # Display summarized DataFrame (`summary_df`) with per-epoch metrics\n",
    "    summary_df = pd.DataFrame(results)\n",
    "    pd.set_option('display.max_rows', None)\n",
    "    display(summary_df.style.set_table_styles([{'selector': 'th', 'props': [('font-weight', 'bold')]}]))\n",
    "\n",
    "    # Create `detailed_df` with per-class metrics in columns, suitable for further analysis\n",
    "    detailed_df = pd.DataFrame(detailed_metrics)\n",
    "    return final_y_true_valid, final_y_pred_proba_valid\n",
    "\n",
    "# Convert labels from dictionaries to multi-label binary tensors\n",
    "def convert_labels_to_tensor(labels):\n",
    "    labels_tensor = torch.zeros(len(labels), 3)  # Three classes: abnormal, ACL, meniscus\n",
    "    for i, label_dict in enumerate(labels):\n",
    "        labels_tensor[i, 0] = label_dict['abnormal']\n",
    "        labels_tensor[i, 1] = label_dict['acl']\n",
    "        labels_tensor[i, 2] = label_dict['meniscus']\n",
    "    return labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XbmQYxR5QR5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train DataLoader - Batch Size: 8, Num Workers: 4\n",
      "Validation DataLoader - Batch Size: 8, Num Workers: 4\n"
     ]
    }
   ],
   "source": [
    "# Function to convert a list of dictionaries to a tensor\n",
    "def convert_labels_to_tensor(label_dicts):\n",
    "    # Convert each dictionary's values into a list and create a tensor from it\n",
    "    labels_as_lists = [list(label.values()) for label in label_dicts]\n",
    "    return torch.tensor(labels_as_lists, dtype=torch.float32)\n",
    "\n",
    "# Apply this to train and validation labels\n",
    "train_labels_tensor = convert_labels_to_tensor(train_labels)\n",
    "valid_labels_tensor = convert_labels_to_tensor(valid_labels)\n",
    "\n",
    "# Create TensorDatasets for training and validation data\n",
    "train_tensor = TensorDataset(torch.tensor(train_data).float(), train_labels_tensor)\n",
    "valid_tensor = TensorDataset(torch.tensor(valid_data).float(), valid_labels_tensor)\n",
    "\n",
    "# Adjust batch size to avoid high memory usage\n",
    "batch_size = 8  # Adjust based on available memory\n",
    "\n",
    "# Create DataLoaders with updated parameters\n",
    "train_loader = DataLoader(\n",
    "    train_tensor,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=4  # Adjust based on your CPU cores and memory\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_tensor,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True,\n",
    "    num_workers=4\n",
    ")\n",
    "\n",
    "# Print out DataLoader settings for confirmation\n",
    "print(f\"Train DataLoader - Batch Size: {batch_size}, Num Workers: 4\")\n",
    "print(f\"Validation DataLoader - Batch Size: {batch_size}, Num Workers: 4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "vgKp7wZlQTxK"
   },
   "outputs": [],
   "source": [
    "def plot_roc_and_calculate_auc(y_true, y_pred_proba, class_names=[\"abnormal\", \"acl\", \"meniscus\"]):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    auc_scores = {}\n",
    "\n",
    "    for i in range(y_true.shape[1]):  # Assuming y_true and y_pred_proba are (num_samples, num_classes)\n",
    "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        auc_scores[class_names[i]] = roc_auc  # Store AUC for each class\n",
    "\n",
    "        plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
    "\n",
    "    # Plot the random classifier line\n",
    "    plt.plot([0, 1], [0, 1], \"k--\")\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve for Each Class\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print AUC scores for each class\n",
    "    print(\"AUC Scores for each class:\")\n",
    "    for class_name, auc_score in auc_scores.items():\n",
    "        print(f\"{class_name}: {auc_score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "srhCDbAYQWCs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1610612736 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m ResNet3D(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, optimizer_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Run the training and evaluation function and capture the outputs\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m y_true_valid_np, y_pred_proba_valid_np \u001b[38;5;241m=\u001b[39m train_and_evaluate(model, train_loader, valid_loader, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true_valid_np shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_true_valid_np\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred_proba_valid_np shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_pred_proba_valid_np\u001b[38;5;241m.\u001b[39mshape)\n",
      "Cell \u001b[1;32mIn[16], line 85\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, train_loader, valid_loader, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     84\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 85\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(inputs)\n\u001b[0;32m     86\u001b[0m     loss \u001b[38;5;241m=\u001b[39m calculate_loss(outputs, labels)\n\u001b[0;32m     87\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[16], line 15\u001b[0m, in \u001b[0;36mResNet3D.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresnet3d(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torchvision\\models\\video\\resnet.py:251\u001b[0m, in \u001b[0;36mVideoResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 251\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[0;32m    253\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer1(x)\n\u001b[0;32m    254\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer2(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:725\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 725\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\sml\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:720\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    708\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    710\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    711\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    718\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    719\u001b[0m     )\n\u001b[1;32m--> 720\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[0;32m    721\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    722\u001b[0m )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1610612736 bytes."
     ]
    }
   ],
   "source": [
    "model = ResNet3D(pretrained=True, num_classes=3, optimizer_type=\"adam\")\n",
    "\n",
    "# Run the training and evaluation function and capture the outputs\n",
    "y_true_valid_np, y_pred_proba_valid_np = train_and_evaluate(model, train_loader, valid_loader, epochs=1, learning_rate=0.1)\n",
    "\n",
    "print(\"y_true_valid_np shape:\", y_true_valid_np.shape)\n",
    "print(\"y_pred_proba_valid_np shape:\", y_pred_proba_valid_np.shape)\n",
    "\n",
    "# Directly call the function with `y_true_valid_np` and `y_pred_proba_valid_np`\n",
    "plot_roc_and_calculate_auc(y_true_valid_np, y_pred_proba_valid_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
