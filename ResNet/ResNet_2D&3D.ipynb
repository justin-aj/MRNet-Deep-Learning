{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXHBVjd3CWir",
        "outputId": "e0db28e9-70ce-4ade-c8ea-d4a9693f5ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHx7t_2lB5eu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.ndimage import zoom\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import warnings\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss, roc_curve, auc\n",
        "import timm\n",
        "from timm import create_model\n",
        "import torchvision.models as models\n",
        "import torchio as tio\n",
        "from torchio import SubjectsLoader\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Input, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ],
      "metadata": {
        "id": "J5h9KshxDvyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths based on your directory structure\n",
        "main_dir = \"/content/drive/My Drive/DATASET_MRNET/MRNet-v1.0\"\n",
        "train_path = os.path.join(main_dir, \"train\")\n",
        "valid_path = os.path.join(main_dir, \"valid\")"
      ],
      "metadata": {
        "id": "6zCrIaJmCO1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load labels from CSV files\n",
        "train_abnormal = pd.read_csv(os.path.join(main_dir, \"train-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "train_acl = pd.read_csv(os.path.join(main_dir, \"train-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "train_meniscus = pd.read_csv(os.path.join(main_dir, \"train-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "\n",
        "valid_abnormal = pd.read_csv(os.path.join(main_dir, \"valid-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "valid_acl = pd.read_csv(os.path.join(main_dir, \"valid-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "valid_meniscus = pd.read_csv(os.path.join(main_dir, \"valid-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()"
      ],
      "metadata": {
        "id": "Y49PrVAXCcT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to resize the depth of a scan to a target depth\n",
        "def resize_depth(scan, target_depth):\n",
        "    depth_factor = target_depth / scan.shape[0]\n",
        "    return zoom(scan, (depth_factor, 1, 1), order=1)"
      ],
      "metadata": {
        "id": "QMYpgbOtCcWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad a scan to a target shape\n",
        "def pad_to_shape(scan, target_shape):\n",
        "    padded_scan = np.zeros(target_shape, dtype=scan.dtype)\n",
        "    min_d, min_h, min_w = min(scan.shape[0], target_shape[0]), min(scan.shape[1], target_shape[1]), min(scan.shape[2], target_shape[2])\n",
        "    padded_scan[:min_d, :min_h, :min_w] = scan[:min_d, :min_h, :min_w]\n",
        "    return padded_scan"
      ],
      "metadata": {
        "id": "4VREJuQnCcYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load a specific range of MRI data with labels\n",
        "\n",
        "def load_mri_data(data_type=\"train\", start_idx=0, end_idx=9, target_shape=(48, 256, 256), target_depth=48):\n",
        "    \"\"\"\n",
        "    Loads MRI data from a specified range and resizes/pads each scan to a target shape.\n",
        "    Parameters:\n",
        "    - data_type: \"train\" or \"valid\"\n",
        "    - start_idx, end_idx: Range of file indices to load (e.g., 0 to 9 for train, 1130 to 1249 for valid)\n",
        "    - target_shape: Target shape for each scan after resizing and padding\n",
        "    - target_depth: Target depth for each scan to ensure consistent depth\n",
        "    \"\"\"\n",
        "    # Set data path and range\n",
        "    data_path = train_path if data_type == \"train\" else valid_path\n",
        "    axial_path, coronal_path, sagittal_path = Path(data_path) / \"axial\", Path(data_path) / \"coronal\", Path(data_path) / \"sagittal\"\n",
        "\n",
        "    # Select the appropriate labels dictionary based on data type\n",
        "    abnormal_labels = train_abnormal if data_type == \"train\" else valid_abnormal\n",
        "    acl_labels = train_acl if data_type == \"train\" else valid_acl\n",
        "    meniscus_labels = train_meniscus if data_type == \"train\" else valid_meniscus\n",
        "\n",
        "    # Initialize lists to store data and labels\n",
        "    mri_data, labels = [], []\n",
        "\n",
        "    # Load each MRI scan within the specified range\n",
        "    for i in range(start_idx, end_idx + 1):\n",
        "        # Generate file name with zero-padded format (e.g., 0000, 0001, ...)\n",
        "        file_name = f\"{i:04}.npy\"\n",
        "\n",
        "        # Load and process each view with resizing and padding\n",
        "        axial_scan = pad_to_shape(resize_depth(np.load(axial_path / file_name), target_depth), target_shape)\n",
        "        coronal_scan = pad_to_shape(resize_depth(np.load(coronal_path / file_name), target_depth), target_shape)\n",
        "        sagittal_scan = pad_to_shape(resize_depth(np.load(sagittal_path / file_name), target_depth), target_shape)\n",
        "\n",
        "        # Combine the three views into one structure (3, depth, height, width)\n",
        "        combined_scan = np.stack([axial_scan, coronal_scan, sagittal_scan], axis=0)\n",
        "        mri_data.append(combined_scan)\n",
        "\n",
        "        # Retrieve actual labels for the current scan\n",
        "        abnormal_label = abnormal_labels.get(i, 0)  # Default to 0 if label is missing\n",
        "        acl_label = acl_labels.get(i, 0)\n",
        "        meniscus_label = meniscus_labels.get(i, 0)\n",
        "\n",
        "        # Append the actual labels\n",
        "        labels.append({\"abnormal\": abnormal_label, \"acl\": acl_label, \"meniscus\": meniscus_label})\n",
        "\n",
        "    return np.array(mri_data), labels"
      ],
      "metadata": {
        "id": "tKUlb1OHCcbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameters for batch loading with exact final index coverage\n",
        "start_indices = list(range(0, 1130, 100))\n",
        "end_indices = [min(start + 99, 1129) for start in start_indices]  # Ensure final batch ends at 1129\n",
        "\n",
        "# Function to load a batch of data given start and end indices\n",
        "def load_batch(start, end):\n",
        "    return load_mri_data(data_type=\"train\", start_idx=start, end_idx=end)\n",
        "\n",
        "# Initialize lists to store all data and labels\n",
        "all_data, all_labels = [], []\n",
        "\n",
        "# Use ThreadPoolExecutor to parallelize data loading for all batches\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Launch parallel tasks for loading each batch\n",
        "    future_to_indices = {executor.submit(load_batch, start, end): (start, end) for start, end in zip(start_indices, end_indices)}\n",
        "\n",
        "    for future in as_completed(future_to_indices):\n",
        "        data, labels = future.result()\n",
        "        all_data.append(data)\n",
        "        all_labels.extend(labels)  # Extend to add lists of labels directly\n",
        "\n",
        "# Concatenate all data batches into a single array\n",
        "train_data = np.concatenate(all_data, axis=0)\n",
        "train_labels = all_labels  # Already extended to combine all label lists\n",
        "\n",
        "# Check the final shape of training data and labels\n",
        "print(\"Final training data shape:\", train_data.shape)  # Expected: (1130, 3, 48, 256, 256)\n",
        "print(\"Final number of training labels:\", len(train_labels))  # Should match the number of samples in train_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcHEhWmwCcdA",
        "outputId": "4ba7e0e3-41e7-4b96-c2dd-24f4fa55d2f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final training data shape: (1130, 3, 48, 256, 256)\n",
            "Final number of training labels: 1130\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the validation data from indices 1130 to 1249\n",
        "valid_data, valid_labels = load_mri_data(data_type=\"valid\", start_idx=1130, end_idx=1249)\n",
        "\n",
        "# Check data shapes and labels\n",
        "print(\"Validation data shape:\", valid_data.shape)  # Expected: (120, 3, 48, 256, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAXC9qgrCcfX",
        "outputId": "2813ba0f-dccf-48c9-9570-2b3cdeba1989"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data shape: (120, 3, 48, 256, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the axial view (index 0)\n",
        "train_data_axial = train_data[:, 0, :, :, :]  # (num_samples, 48, 256, 256)\n",
        "valid_data_axial = valid_data[:, 0, :, :, :]  # (num_samples, 48, 256, 256)\n",
        "\n",
        "# Reshape to 2D projections by averaging along the depth dimension\n",
        "train_data_reshaped = train_data_axial.mean(axis=1)  # Average along depth for projection\n",
        "valid_data_reshaped = valid_data_axial.mean(axis=1)\n",
        "\n",
        "# Normalize to [0, 1] range\n",
        "train_data_normalized = train_data_reshaped / 255.0\n",
        "valid_data_normalized = valid_data_reshaped / 255.0\n",
        "\n",
        "# Final shapes for PyTorch: (batch_size, channels, height, width)\n",
        "# Add channel dimension as the first axis\n",
        "train_data_final = torch.tensor(train_data_normalized, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
        "valid_data_final = torch.tensor(valid_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Repeat the channel dimension to create 3 channels\n",
        "train_data_final = train_data_final.repeat(1, 3, 1, 1)\n",
        "valid_data_final = valid_data_final.repeat(1, 3, 1, 1)\n",
        "\n",
        "print(\"Train data shape:\", train_data_final.shape)  # Expected: (1130, 3, 256, 256)\n",
        "print(\"Validation data shape:\", valid_data_final.shape)  # Expected: (120, 3, 256, 256)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyHFjx35Gt6Y",
        "outputId": "72c2f081-5942-44c4-b8ec-14e18b1c5e7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data shape: torch.Size([1130, 3, 256, 256])\n",
            "Validation data shape: torch.Size([120, 3, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform labels into binary matrices for multi-label classification\n",
        "# Each label dictionary is converted into a list of binary values for each class\n",
        "train_labels_matrix = [[lbl['abnormal'], lbl['acl'], lbl['meniscus']] for lbl in train_labels]\n",
        "valid_labels_matrix = [[lbl['abnormal'], lbl['acl'], lbl['meniscus']] for lbl in valid_labels]\n",
        "\n",
        "# Convert label matrices to PyTorch tensors\n",
        "# Using float32 because loss functions like BCEWithLogitsLoss expect float inputs\n",
        "train_labels_encoded = torch.tensor(train_labels_matrix, dtype=torch.float32, device=device)  # Send directly to device\n",
        "valid_labels_encoded = torch.tensor(valid_labels_matrix, dtype=torch.float32, device=device)  # Send directly to device\n",
        "\n",
        "# Verify tensor shapes and ensure the data has been prepared correctly\n",
        "print(\"Train labels shape:\", train_labels_encoded.shape)  # Expected: (1130, 3)\n",
        "print(\"Validation labels shape:\", valid_labels_encoded.shape)  # Expected: (120, 3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA_X4l-7C26k",
        "outputId": "cc50ecb4-51b9-4a3b-beb7-a8cb84ba7023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train labels shape: torch.Size([1130, 3])\n",
            "Validation labels shape: torch.Size([120, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MRIModel(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(MRIModel, self).__init__()\n",
        "        self.backbone = timm.create_model('resnet200d', pretrained=True, num_classes=0)\n",
        "        self.fc1 = nn.Linear(self.backbone.num_features, 512)  # backbone.num_features should be 2048\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc_out = nn.Linear(256, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.fc_out(x)\n",
        "        return self.sigmoid(x)\n"
      ],
      "metadata": {
        "id": "_sbNRShTTtPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_data_final, train_labels_encoded)\n",
        "valid_dataset = TensorDataset(valid_data_final, valid_labels_encoded)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "for data, labels in train_loader:\n",
        "    print(f\"Data batch shape: {data.shape}\")  # Expected: (batch_size, 3, 256, 256)\n",
        "    print(f\"Labels batch shape: {labels.shape}\")  # Expected: (batch_size, 3)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc5_rBnsTvsD",
        "outputId": "5840e49e-7856-4b1c-82d1-cde264fb4bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data batch shape: torch.Size([16, 3, 256, 256])\n",
            "Labels batch shape: torch.Size([16, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = MRIModel(num_classes=3).to(device)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "id": "9iYLVRjNTyW4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data, labels in loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "\n",
        "        # Store predictions and true labels for metrics\n",
        "        all_preds.append(outputs.detach().cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    all_preds_binary = (all_preds > 0.5).int()\n",
        "    accuracy = accuracy_score(all_labels.numpy(), all_preds_binary.numpy())\n",
        "    precision = precision_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "\n",
        "    return running_loss / len(loader.dataset), accuracy, precision, recall\n",
        "\n"
      ],
      "metadata": {
        "id": "ilEI5zYnT01w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Store predictions and true labels for metric calculation\n",
        "            all_preds.append(outputs.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    all_preds_binary = (all_preds > 0.5).int()\n",
        "    accuracy = accuracy_score(all_labels.numpy(), all_preds_binary.numpy())\n",
        "    precision = precision_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "\n",
        "    return running_loss / len(loader.dataset), accuracy, precision, recall\n"
      ],
      "metadata": {
        "id": "HU8YJAKIT3gV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    train_loss, train_accuracy, train_precision, train_recall = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device\n",
        "    )\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_accuracy, val_precision, val_recall = validate_epoch(\n",
        "        model, valid_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUByb0yET7BO",
        "outputId": "919b5ba7-93e9-4b71-f561-6af935849a35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.5491, Accuracy: 0.3779, Precision: 0.3553, Recall: 0.3401\n",
            "Validation Loss: 0.6341, Accuracy: 0.1583, Precision: 0.4306, Recall: 0.3590\n",
            "Epoch 2/20\n",
            "Train Loss: 0.4359, Accuracy: 0.4292, Precision: 0.6008, Recall: 0.4415\n",
            "Validation Loss: 0.6245, Accuracy: 0.2250, Precision: 0.4870, Recall: 0.4644\n",
            "Epoch 3/20\n",
            "Train Loss: 0.3037, Accuracy: 0.6389, Precision: 0.8191, Recall: 0.6945\n",
            "Validation Loss: 0.6581, Accuracy: 0.2583, Precision: 0.7466, Recall: 0.6712\n",
            "Epoch 4/20\n",
            "Train Loss: 0.1930, Accuracy: 0.7823, Precision: 0.8844, Recall: 0.8451\n",
            "Validation Loss: 0.6194, Accuracy: 0.3500, Precision: 0.7580, Recall: 0.6391\n",
            "Epoch 5/20\n",
            "Train Loss: 0.1308, Accuracy: 0.8425, Precision: 0.9147, Recall: 0.9164\n",
            "Validation Loss: 0.8839, Accuracy: 0.3667, Precision: 0.7480, Recall: 0.7464\n",
            "Epoch 6/20\n",
            "Train Loss: 0.0969, Accuracy: 0.9018, Precision: 0.9525, Recall: 0.9313\n",
            "Validation Loss: 0.7643, Accuracy: 0.3250, Precision: 0.7335, Recall: 0.6751\n",
            "Epoch 7/20\n",
            "Train Loss: 0.0646, Accuracy: 0.9345, Precision: 0.9712, Recall: 0.9538\n",
            "Validation Loss: 1.0538, Accuracy: 0.3667, Precision: 0.7835, Recall: 0.6024\n",
            "Epoch 8/20\n",
            "Train Loss: 0.0437, Accuracy: 0.9611, Precision: 0.9819, Recall: 0.9744\n",
            "Validation Loss: 0.9267, Accuracy: 0.3917, Precision: 0.7149, Recall: 0.8131\n",
            "Epoch 9/20\n",
            "Train Loss: 0.0407, Accuracy: 0.9637, Precision: 0.9844, Recall: 0.9788\n",
            "Validation Loss: 1.2459, Accuracy: 0.4000, Precision: 0.7537, Recall: 0.7687\n",
            "Epoch 10/20\n",
            "Train Loss: 0.0380, Accuracy: 0.9602, Precision: 0.9825, Recall: 0.9789\n",
            "Validation Loss: 0.9063, Accuracy: 0.4333, Precision: 0.7704, Recall: 0.7913\n",
            "Epoch 11/20\n",
            "Train Loss: 0.0589, Accuracy: 0.9354, Precision: 0.9713, Recall: 0.9616\n",
            "Validation Loss: 0.9694, Accuracy: 0.3250, Precision: 0.7605, Recall: 0.6847\n",
            "Epoch 12/20\n",
            "Train Loss: 0.0324, Accuracy: 0.9619, Precision: 0.9843, Recall: 0.9723\n",
            "Validation Loss: 1.1261, Accuracy: 0.3500, Precision: 0.7513, Recall: 0.6634\n",
            "Epoch 13/20\n",
            "Train Loss: 0.0238, Accuracy: 0.9779, Precision: 0.9893, Recall: 0.9870\n",
            "Validation Loss: 0.9199, Accuracy: 0.3917, Precision: 0.7459, Recall: 0.7699\n",
            "Epoch 14/20\n",
            "Train Loss: 0.0139, Accuracy: 0.9841, Precision: 0.9942, Recall: 0.9923\n",
            "Validation Loss: 1.0984, Accuracy: 0.3167, Precision: 0.7281, Recall: 0.7767\n",
            "Epoch 15/20\n",
            "Train Loss: 0.0221, Accuracy: 0.9788, Precision: 0.9904, Recall: 0.9896\n",
            "Validation Loss: 1.0497, Accuracy: 0.3583, Precision: 0.7866, Recall: 0.6636\n",
            "Epoch 16/20\n",
            "Train Loss: 0.0270, Accuracy: 0.9743, Precision: 0.9863, Recall: 0.9845\n",
            "Validation Loss: 1.0722, Accuracy: 0.3250, Precision: 0.7574, Recall: 0.7507\n",
            "Epoch 17/20\n",
            "Train Loss: 0.0246, Accuracy: 0.9743, Precision: 0.9858, Recall: 0.9859\n",
            "Validation Loss: 1.1319, Accuracy: 0.3833, Precision: 0.7845, Recall: 0.6420\n",
            "Epoch 18/20\n",
            "Train Loss: 0.0108, Accuracy: 0.9903, Precision: 0.9975, Recall: 0.9915\n",
            "Validation Loss: 1.1616, Accuracy: 0.3333, Precision: 0.7454, Recall: 0.7514\n",
            "Epoch 19/20\n",
            "Train Loss: 0.0099, Accuracy: 0.9929, Precision: 0.9944, Recall: 0.9964\n",
            "Validation Loss: 1.2466, Accuracy: 0.3000, Precision: 0.7410, Recall: 0.6615\n",
            "Epoch 20/20\n",
            "Train Loss: 0.0142, Accuracy: 0.9903, Precision: 0.9939, Recall: 0.9927\n",
            "Validation Loss: 1.1957, Accuracy: 0.3250, Precision: 0.7485, Recall: 0.6604\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define data transformations with augmentation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomResizedCrop(256, scale=(0.9, 1.0)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Update your dataset to apply transformations\n",
        "class MRIDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, labels, transform=None):\n",
        "        self.data = data  # Tensor of images\n",
        "        self.labels = labels  # Tensor of labels\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = self.data[idx]\n",
        "        label = self.labels[idx]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "8QHQVc3dfKx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data, labels in loader:\n",
        "        # Move data and labels to device here\n",
        "        data = data.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "\n",
        "        # Store predictions and true labels for metrics\n",
        "        all_preds.append(outputs.detach().cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    all_preds_binary = (all_preds > 0.5).int()\n",
        "    accuracy = accuracy_score(all_labels.numpy(), all_preds_binary.numpy())\n",
        "    precision = precision_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "\n",
        "    return running_loss / len(loader.dataset), accuracy, precision, recall\n",
        "\n"
      ],
      "metadata": {
        "id": "QrA-_H2yhNrR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels_encoded = torch.tensor(train_labels_matrix, dtype=torch.float32)\n",
        "valid_labels_encoded = torch.tensor(valid_labels_matrix, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "ViWNFiwBhN04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create datasets\n",
        "train_dataset = MRIDataset(train_data_final, train_labels_encoded, transform=train_transforms)\n",
        "valid_dataset = MRIDataset(valid_data_final, valid_labels_encoded)\n",
        "\n",
        "# Create data loaders with num_workers\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4, pin_memory=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=4, pin_memory=True)"
      ],
      "metadata": {
        "id": "ZSMfcS9ehN2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the model with dropout\n",
        "class MRIModel(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(MRIModel, self).__init__()\n",
        "        self.backbone = timm.create_model('resnet50', pretrained=True, num_classes=0)\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(self.backbone.num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc2 = nn.Sequential(\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "        self.fc_out = nn.Linear(256, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.fc_out(x)\n",
        "        return self.sigmoid(x)"
      ],
      "metadata": {
        "id": "PgoyhxWWhN51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize the model, criterion, optimizer, and scheduler\n",
        "model = MRIModel(num_classes=3).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3)"
      ],
      "metadata": {
        "id": "8ZqIfuKYhN8v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    train_loss, train_accuracy, train_precision, train_recall = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device\n",
        "    )\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_accuracy, val_precision, val_recall = validate_epoch(\n",
        "        model, valid_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    # Step scheduler\n",
        "    scheduler.step(val_loss)\n",
        "\n",
        "    # Log the current learning rate\n",
        "    current_lr = optimizer.param_groups[0]['lr']\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Learning Rate: {current_lr:.6f}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OrTc9EIEfXzv",
        "outputId": "27e2ebac-6402-43f4-8fc5-be2428a6d641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.5975, Accuracy: 0.3655, Precision: 0.4688, Recall: 0.3616\n",
            "Validation Loss: 0.6817, Accuracy: 0.1667, Precision: 0.2639, Recall: 0.3333\n",
            "\n",
            "Epoch 2/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.5371, Accuracy: 0.3779, Precision: 0.3754, Recall: 0.3392\n",
            "Validation Loss: 0.6494, Accuracy: 0.1667, Precision: 0.2639, Recall: 0.3333\n",
            "\n",
            "Epoch 3/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.5276, Accuracy: 0.3823, Precision: 0.4249, Recall: 0.3392\n",
            "Validation Loss: 0.6932, Accuracy: 0.1667, Precision: 0.2639, Recall: 0.3333\n",
            "\n",
            "Epoch 4/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.5281, Accuracy: 0.3823, Precision: 0.4152, Recall: 0.3392\n",
            "Validation Loss: 0.6522, Accuracy: 0.1667, Precision: 0.2639, Recall: 0.3333\n",
            "\n",
            "Epoch 5/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.5191, Accuracy: 0.3832, Precision: 0.4499, Recall: 0.3442\n",
            "Validation Loss: 0.6667, Accuracy: 0.1667, Precision: 0.2639, Recall: 0.3333\n",
            "\n",
            "Epoch 6/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.4978, Accuracy: 0.4035, Precision: 0.4738, Recall: 0.3946\n",
            "Validation Loss: 0.6262, Accuracy: 0.2417, Precision: 0.5337, Recall: 0.4423\n",
            "\n",
            "Epoch 7/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.4761, Accuracy: 0.3973, Precision: 0.4584, Recall: 0.4441\n",
            "Validation Loss: 0.5921, Accuracy: 0.2333, Precision: 0.4810, Recall: 0.5285\n",
            "\n",
            "Epoch 8/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.4551, Accuracy: 0.4336, Precision: 0.4769, Recall: 0.4724\n",
            "Validation Loss: 0.5662, Accuracy: 0.2583, Precision: 0.4820, Recall: 0.5699\n",
            "\n",
            "Epoch 9/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.4407, Accuracy: 0.4558, Precision: 0.5726, Recall: 0.5169\n",
            "Validation Loss: 0.5829, Accuracy: 0.2917, Precision: 0.4889, Recall: 0.5308\n",
            "\n",
            "Epoch 10/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.4214, Accuracy: 0.4664, Precision: 0.6247, Recall: 0.5202\n",
            "Validation Loss: 0.5853, Accuracy: 0.2833, Precision: 0.4930, Recall: 0.4889\n",
            "\n",
            "Epoch 11/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.4082, Accuracy: 0.4858, Precision: 0.7211, Recall: 0.5360\n",
            "Validation Loss: 0.6026, Accuracy: 0.2667, Precision: 0.4649, Recall: 0.5542\n",
            "\n",
            "Epoch 12/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.4031, Accuracy: 0.4947, Precision: 0.7045, Recall: 0.5360\n",
            "Validation Loss: 0.5203, Accuracy: 0.3167, Precision: 0.8259, Recall: 0.5944\n",
            "\n",
            "Epoch 13/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.3977, Accuracy: 0.5133, Precision: 0.6936, Recall: 0.5673\n",
            "Validation Loss: 0.5881, Accuracy: 0.3083, Precision: 0.8156, Recall: 0.4824\n",
            "\n",
            "Epoch 14/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.3976, Accuracy: 0.4938, Precision: 0.7047, Recall: 0.5634\n",
            "Validation Loss: 0.5437, Accuracy: 0.3167, Precision: 0.8216, Recall: 0.6161\n",
            "\n",
            "Epoch 15/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.3818, Accuracy: 0.5195, Precision: 0.7575, Recall: 0.5919\n",
            "Validation Loss: 0.4860, Accuracy: 0.3417, Precision: 0.7948, Recall: 0.7495\n",
            "\n",
            "Epoch 16/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.3762, Accuracy: 0.5363, Precision: 0.7285, Recall: 0.6403\n",
            "Validation Loss: 0.5338, Accuracy: 0.3333, Precision: 0.7828, Recall: 0.7417\n",
            "\n",
            "Epoch 17/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.3710, Accuracy: 0.5274, Precision: 0.7392, Recall: 0.6016\n",
            "Validation Loss: 0.5850, Accuracy: 0.3083, Precision: 0.7915, Recall: 0.6656\n",
            "\n",
            "Epoch 18/20\n",
            "Learning Rate: 0.000100\n",
            "Train Loss: 0.3546, Accuracy: 0.5522, Precision: 0.7457, Recall: 0.6536\n",
            "Validation Loss: 0.5845, Accuracy: 0.3000, Precision: 0.7881, Recall: 0.6943\n",
            "\n",
            "Epoch 19/20\n",
            "Learning Rate: 0.000010\n",
            "Train Loss: 0.3544, Accuracy: 0.5673, Precision: 0.7498, Recall: 0.6701\n",
            "Validation Loss: 0.5857, Accuracy: 0.3583, Precision: 0.7751, Recall: 0.7340\n",
            "\n",
            "Epoch 20/20\n",
            "Learning Rate: 0.000010\n",
            "Train Loss: 0.3458, Accuracy: 0.5823, Precision: 0.7454, Recall: 0.7126\n",
            "Validation Loss: 0.5411, Accuracy: 0.3833, Precision: 0.7888, Recall: 0.7402\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare subjects for training data\n",
        "train_subjects = []\n",
        "for i in range(len(train_data)):\n",
        "    image_tensor = train_data[i]  # Shape: (C, D, H, W)\n",
        "    label = train_labels[i]\n",
        "    image = tio.ScalarImage(tensor=image_tensor)\n",
        "    subject = tio.Subject(\n",
        "    image=image,\n",
        "    abnormal=label['abnormal'],\n",
        "    acl=label['acl'],\n",
        "    meniscus=label['meniscus']\n",
        "    )\n",
        "    train_subjects.append(subject)\n",
        "\n",
        "# Prepare subjects for validation data\n",
        "valid_subjects = []\n",
        "for i in range(len(valid_data)):\n",
        "    image_tensor = valid_data[i]\n",
        "    label = valid_labels[i]\n",
        "    image = tio.ScalarImage(tensor=image_tensor)\n",
        "    subject = tio.Subject(\n",
        "    image=image,\n",
        "    abnormal=label['abnormal'],\n",
        "    acl=label['acl'],\n",
        "    meniscus=label['meniscus']\n",
        "    )\n",
        "    valid_subjects.append(subject)\n",
        "\n",
        "# Create datasets using tio.SubjectsDataset\n",
        "train_dataset = tio.SubjectsDataset(train_subjects, transform=train_transforms)\n",
        "valid_dataset = tio.SubjectsDataset(valid_subjects, transform=valid_transforms)"
      ],
      "metadata": {
        "id": "xeagv35yjwTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations using TorchIO\n",
        "train_transforms = tio.Compose([\n",
        "    tio.RandomFlip(axes=('LR', 'AP', 'SI'), flip_probability=0.5, include=('image',)),\n",
        "    tio.RandomAffine(scales=(0.9, 1.1), degrees=10, include=('image',)),\n",
        "    tio.ZNormalization(include=('image',)),\n",
        "])\n",
        "\n",
        "valid_transforms = tio.Compose([\n",
        "    tio.ZNormalization(include=('image',)),\n",
        "])"
      ],
      "metadata": {
        "id": "uPLH44T9k2Dn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = SubjectsLoader(train_dataset, batch_size=1, shuffle=True, num_workers=4)\n",
        "valid_loader = SubjectsLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=4)"
      ],
      "metadata": {
        "id": "MNWF2yxWk5g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_dataset[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkZsSGGYxt79",
        "outputId": "415a70fe-b00b-4a54-f90f-7386ea6cf556"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torchio.data.subject.Subject'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MRI3DResNet(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(MRI3DResNet, self).__init__()\n",
        "        # Load a pretrained 3D ResNet\n",
        "        self.backbone = models.video.r3d_18(pretrained=True)\n",
        "        # Modify the first convolutional layer to accept 3 input channels\n",
        "        self.backbone.stem[0] = nn.Conv3d(\n",
        "            in_channels=3,  # Change to 3 channels\n",
        "            out_channels=64,\n",
        "            kernel_size=(3, 7, 7),\n",
        "            stride=(1, 2, 2),\n",
        "            padding=(1, 3, 3),\n",
        "            bias=False\n",
        "        )\n",
        "        # Replace the fully connected layer\n",
        "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
        "        # No activation here because we'll use BCEWithLogitsLoss\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "b3cBRUDWk-jz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()"
      ],
      "metadata": {
        "id": "7zsO9CyAlEMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "XJXMQKEklIyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MRI3DResNet(num_classes=3).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B05mYPVLlMOl",
        "outputId": "a9f13b91-df8f-4ce9-c3ef-78490bb2d356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(train_subjects[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2ROirPoveEu",
        "outputId": "eb2852f0-1d83-4e20-f3e7-68676fd4667e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torchio.data.subject.Subject'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a1n-eAA2fJf",
        "outputId": "2c28d190-9e0c-4112-8882-23469a374551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.5.1+cu121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if not param.requires_grad:\n",
        "        print(f\"Parameter {name} does not require gradient\")"
      ],
      "metadata": {
        "id": "FXHnRfC03WmK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"torchio\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_train_preds = []\n",
        "    all_train_labels = []\n",
        "    for batch in train_loader:\n",
        "        data = batch['image'][tio.DATA].to(device)\n",
        "        labels = torch.tensor(\n",
        "            [batch['abnormal'], batch['acl'], batch['meniscus']],\n",
        "            dtype=torch.float32\n",
        "        ).T.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "\n",
        "        # Collect predictions and labels for metrics\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        preds = (outputs > 0.5).float()\n",
        "        all_train_preds.append(preds.cpu())\n",
        "        all_train_labels.append(labels.cpu())\n",
        "\n",
        "    # Compute training metrics\n",
        "    all_train_preds = torch.cat(all_train_preds)\n",
        "    all_train_labels = torch.cat(all_train_labels)\n",
        "    train_accuracy = accuracy_score(all_train_labels.numpy(), all_train_preds.numpy())\n",
        "    train_precision = precision_score(all_train_labels.numpy(), all_train_preds.numpy(), average='weighted', zero_division=0)\n",
        "    train_recall = recall_score(all_train_labels.numpy(), all_train_preds.numpy(), average='weighted', zero_division=0)\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Training Loss: {epoch_loss:.4f}, Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    all_val_preds = []\n",
        "    all_val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for batch in valid_loader:\n",
        "            data = batch['image'][tio.DATA].to(device)\n",
        "            labels = torch.tensor(\n",
        "                [batch['abnormal'], batch['acl'], batch['meniscus']],\n",
        "                dtype=torch.float32\n",
        "            ).T.to(device)\n",
        "\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            val_running_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Collect predictions and labels for metrics\n",
        "            outputs = torch.sigmoid(outputs)\n",
        "            preds = (outputs > 0.5).float()\n",
        "            all_val_preds.append(preds.cpu())\n",
        "            all_val_labels.append(labels.cpu())\n",
        "\n",
        "    # Compute validation metrics\n",
        "    all_val_preds = torch.cat(all_val_preds)\n",
        "    all_val_labels = torch.cat(all_val_labels)\n",
        "    val_accuracy = accuracy_score(all_val_labels.numpy(), all_val_preds.numpy())\n",
        "    val_precision = precision_score(all_val_labels.numpy(), all_val_preds.numpy(), average='weighted', zero_division=0)\n",
        "    val_recall = recall_score(all_val_labels.numpy(), all_val_preds.numpy(), average='weighted', zero_division=0)\n",
        "    val_loss = val_running_loss / len(valid_dataset)\n",
        "\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418
        },
        "id": "zYRUMpPslOVM",
        "outputId": "eee3f02d-fdc3-4cff-8fb8-6331d3c1c9d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "Training Loss: 0.7019, Accuracy: 0.0000, Precision: 0.0919, Recall: 0.2615\n",
            "Validation Loss: 0.8315, Accuracy: 0.0000, Precision: 0.4497, Recall: 0.2836\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-144-e3e6bda8e012>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mall_train_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mall_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDATA\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         labels = torch.tensor(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1447\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1448\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1449\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1241\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                     \u001b[0mtimeout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeadline\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonotonic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mpoll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_readable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36m_poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_poll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    929\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    930\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m                 \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileobj\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m             \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}