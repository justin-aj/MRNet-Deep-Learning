{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXHBVjd3CWir",
        "outputId": "e0db28e9-70ce-4ade-c8ea-d4a9693f5ed7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHx7t_2lB5eu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.ndimage import zoom\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6zCrIaJmCO1l"
      },
      "outputs": [],
      "source": [
        "# Define paths based on your directory structure\n",
        "main_dir = \"/content/drive/My Drive/DATASET_MRNET/MRNet-v1.0\"\n",
        "train_path = os.path.join(main_dir, \"train\")\n",
        "valid_path = os.path.join(main_dir, \"valid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y49PrVAXCcT2"
      },
      "outputs": [],
      "source": [
        "# Load labels from CSV files\n",
        "train_abnormal = pd.read_csv(os.path.join(main_dir, \"train-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "train_acl = pd.read_csv(os.path.join(main_dir, \"train-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "train_meniscus = pd.read_csv(os.path.join(main_dir, \"train-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "\n",
        "valid_abnormal = pd.read_csv(os.path.join(main_dir, \"valid-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "valid_acl = pd.read_csv(os.path.join(main_dir, \"valid-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "valid_meniscus = pd.read_csv(os.path.join(main_dir, \"valid-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMYpgbOtCcWP"
      },
      "outputs": [],
      "source": [
        "# Function to resize the depth of a scan to a target depth\n",
        "def resize_depth(scan, target_depth):\n",
        "    depth_factor = target_depth / scan.shape[0]\n",
        "    return zoom(scan, (depth_factor, 1, 1), order=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4VREJuQnCcYk"
      },
      "outputs": [],
      "source": [
        "# Function to pad a scan to a target shape\n",
        "def pad_to_shape(scan, target_shape):\n",
        "    padded_scan = np.zeros(target_shape, dtype=scan.dtype)\n",
        "    min_d, min_h, min_w = min(scan.shape[0], target_shape[0]), min(scan.shape[1], target_shape[1]), min(scan.shape[2], target_shape[2])\n",
        "    padded_scan[:min_d, :min_h, :min_w] = scan[:min_d, :min_h, :min_w]\n",
        "    return padded_scan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKUlb1OHCcbL"
      },
      "outputs": [],
      "source": [
        "# Function to load a specific range of MRI data with labels\n",
        "\n",
        "def load_mri_data(data_type=\"train\", start_idx=0, end_idx=9, target_shape=(48, 256, 256), target_depth=48):\n",
        "    \"\"\"\n",
        "    Loads MRI data from a specified range and resizes/pads each scan to a target shape.\n",
        "    Parameters:\n",
        "    - data_type: \"train\" or \"valid\"\n",
        "    - start_idx, end_idx: Range of file indices to load (e.g., 0 to 9 for train, 1130 to 1249 for valid)\n",
        "    - target_shape: Target shape for each scan after resizing and padding\n",
        "    - target_depth: Target depth for each scan to ensure consistent depth\n",
        "    \"\"\"\n",
        "    # Set data path and range\n",
        "    data_path = train_path if data_type == \"train\" else valid_path\n",
        "    axial_path, coronal_path, sagittal_path = Path(data_path) / \"axial\", Path(data_path) / \"coronal\", Path(data_path) / \"sagittal\"\n",
        "\n",
        "    # Select the appropriate labels dictionary based on data type\n",
        "    abnormal_labels = train_abnormal if data_type == \"train\" else valid_abnormal\n",
        "    acl_labels = train_acl if data_type == \"train\" else valid_acl\n",
        "    meniscus_labels = train_meniscus if data_type == \"train\" else valid_meniscus\n",
        "\n",
        "    # Initialize lists to store data and labels\n",
        "    mri_data, labels = [], []\n",
        "\n",
        "    # Load each MRI scan within the specified range\n",
        "    for i in range(start_idx, end_idx + 1):\n",
        "        # Generate file name with zero-padded format (e.g., 0000, 0001, ...)\n",
        "        file_name = f\"{i:04}.npy\"\n",
        "\n",
        "        # Load and process each view with resizing and padding\n",
        "        axial_scan = pad_to_shape(resize_depth(np.load(axial_path / file_name), target_depth), target_shape)\n",
        "        coronal_scan = pad_to_shape(resize_depth(np.load(coronal_path / file_name), target_depth), target_shape)\n",
        "        sagittal_scan = pad_to_shape(resize_depth(np.load(sagittal_path / file_name), target_depth), target_shape)\n",
        "\n",
        "        # Combine the three views into one structure (3, depth, height, width)\n",
        "        combined_scan = np.stack([axial_scan, coronal_scan, sagittal_scan], axis=0)\n",
        "        mri_data.append(combined_scan)\n",
        "\n",
        "        # Retrieve actual labels for the current scan\n",
        "        abnormal_label = abnormal_labels.get(i, 0)  # Default to 0 if label is missing\n",
        "        acl_label = acl_labels.get(i, 0)\n",
        "        meniscus_label = meniscus_labels.get(i, 0)\n",
        "\n",
        "        # Append the actual labels\n",
        "        labels.append({\"abnormal\": abnormal_label, \"acl\": acl_label, \"meniscus\": meniscus_label})\n",
        "\n",
        "    return np.array(mri_data), labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qcHEhWmwCcdA",
        "outputId": "4ba7e0e3-41e7-4b96-c2dd-24f4fa55d2f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final training data shape: (1130, 3, 48, 256, 256)\n",
            "Final number of training labels: 1130\n"
          ]
        }
      ],
      "source": [
        "# Define the parameters for batch loading with exact final index coverage\n",
        "start_indices = list(range(0, 1130, 100))\n",
        "end_indices = [min(start + 99, 1129) for start in start_indices]  # Ensure final batch ends at 1129\n",
        "\n",
        "# Function to load a batch of data given start and end indices\n",
        "def load_batch(start, end):\n",
        "    return load_mri_data(data_type=\"train\", start_idx=start, end_idx=end)\n",
        "\n",
        "# Initialize lists to store all data and labels\n",
        "all_data, all_labels = [], []\n",
        "\n",
        "# Use ThreadPoolExecutor to parallelize data loading for all batches\n",
        "with ThreadPoolExecutor() as executor:\n",
        "    # Launch parallel tasks for loading each batch\n",
        "    future_to_indices = {executor.submit(load_batch, start, end): (start, end) for start, end in zip(start_indices, end_indices)}\n",
        "\n",
        "    for future in as_completed(future_to_indices):\n",
        "        data, labels = future.result()\n",
        "        all_data.append(data)\n",
        "        all_labels.extend(labels)  # Extend to add lists of labels directly\n",
        "\n",
        "# Concatenate all data batches into a single array\n",
        "train_data = np.concatenate(all_data, axis=0)\n",
        "train_labels = all_labels  # Already extended to combine all label lists\n",
        "\n",
        "# Check the final shape of training data and labels\n",
        "print(\"Final training data shape:\", train_data.shape)  # Expected: (1130, 3, 48, 256, 256)\n",
        "print(\"Final number of training labels:\", len(train_labels))  # Should match the number of samples in train_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAXC9qgrCcfX",
        "outputId": "2813ba0f-dccf-48c9-9570-2b3cdeba1989"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation data shape: (120, 3, 48, 256, 256)\n"
          ]
        }
      ],
      "source": [
        "# Load the validation data from indices 1130 to 1249\n",
        "valid_data, valid_labels = load_mri_data(data_type=\"valid\", start_idx=1130, end_idx=1249)\n",
        "\n",
        "# Check data shapes and labels\n",
        "print(\"Validation data shape:\", valid_data.shape)  # Expected: (120, 3, 48, 256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyHFjx35Gt6Y",
        "outputId": "72c2f081-5942-44c4-b8ec-14e18b1c5e7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train data shape: torch.Size([1130, 3, 256, 256])\n",
            "Validation data shape: torch.Size([120, 3, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "# Select the axial view (index 0)\n",
        "train_data_axial = train_data[:, 0, :, :, :]  # (num_samples, 48, 256, 256)\n",
        "valid_data_axial = valid_data[:, 0, :, :, :]  # (num_samples, 48, 256, 256)\n",
        "\n",
        "# Reshape to 2D projections by averaging along the depth dimension\n",
        "train_data_reshaped = train_data_axial.mean(axis=1)  # Average along depth for projection\n",
        "valid_data_reshaped = valid_data_axial.mean(axis=1)\n",
        "\n",
        "# Normalize to [0, 1] range\n",
        "train_data_normalized = train_data_reshaped / 255.0\n",
        "valid_data_normalized = valid_data_reshaped / 255.0\n",
        "\n",
        "# Final shapes for PyTorch: (batch_size, channels, height, width)\n",
        "# Add channel dimension as the first axis\n",
        "train_data_final = torch.tensor(train_data_normalized, dtype=torch.float32).unsqueeze(1)  # Add channel dimension\n",
        "valid_data_final = torch.tensor(valid_data_normalized, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "# Repeat the channel dimension to create 3 channels\n",
        "train_data_final = train_data_final.repeat(1, 3, 1, 1)\n",
        "valid_data_final = valid_data_final.repeat(1, 3, 1, 1)\n",
        "\n",
        "print(\"Train data shape:\", train_data_final.shape)  # Expected: (1130, 3, 256, 256)\n",
        "print(\"Validation data shape:\", valid_data_final.shape)  # Expected: (120, 3, 256, 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA_X4l-7C26k",
        "outputId": "cc50ecb4-51b9-4a3b-beb7-a8cb84ba7023"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train labels shape: torch.Size([1130, 3])\n",
            "Validation labels shape: torch.Size([120, 3])\n"
          ]
        }
      ],
      "source": [
        "# Transform labels into binary matrices for multi-label classification\n",
        "# Each label dictionary is converted into a list of binary values for each class\n",
        "train_labels_matrix = [[lbl['abnormal'], lbl['acl'], lbl['meniscus']] for lbl in train_labels]\n",
        "valid_labels_matrix = [[lbl['abnormal'], lbl['acl'], lbl['meniscus']] for lbl in valid_labels]\n",
        "\n",
        "# Convert label matrices to PyTorch tensors\n",
        "# Using float32 because loss functions like BCEWithLogitsLoss expect float inputs\n",
        "train_labels_encoded = torch.tensor(train_labels_matrix, dtype=torch.float32, device=device)  # Send directly to device\n",
        "valid_labels_encoded = torch.tensor(valid_labels_matrix, dtype=torch.float32, device=device)  # Send directly to device\n",
        "\n",
        "# Verify tensor shapes and ensure the data has been prepared correctly\n",
        "print(\"Train labels shape:\", train_labels_encoded.shape)  # Expected: (1130, 3)\n",
        "print(\"Validation labels shape:\", valid_labels_encoded.shape)  # Expected: (120, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sbNRShTTtPR"
      },
      "outputs": [],
      "source": [
        "class MRIModel(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super(MRIModel, self).__init__()\n",
        "        self.backbone = timm.create_model('resnet200d', pretrained=True, num_classes=0)\n",
        "        self.fc1 = nn.Linear(self.backbone.num_features, 512)  # backbone.num_features should be 2048\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc_out = nn.Linear(256, num_classes)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.fc_out(x)\n",
        "        return self.sigmoid(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vc5_rBnsTvsD",
        "outputId": "5840e49e-7856-4b1c-82d1-cde264fb4bf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data batch shape: torch.Size([16, 3, 256, 256])\n",
            "Labels batch shape: torch.Size([16, 3])\n"
          ]
        }
      ],
      "source": [
        "train_dataset = TensorDataset(train_data_final, train_labels_encoded)\n",
        "valid_dataset = TensorDataset(valid_data_final, valid_labels_encoded)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "for data, labels in train_loader:\n",
        "    print(f\"Data batch shape: {data.shape}\")  # Expected: (batch_size, 3, 256, 256)\n",
        "    print(f\"Labels batch shape: {labels.shape}\")  # Expected: (batch_size, 3)\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iYLVRjNTyW4"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = MRIModel(num_classes=3).to(device)\n",
        "criterion = nn.BCELoss()  # Binary Cross-Entropy Loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ilEI5zYnT01w"
      },
      "outputs": [],
      "source": [
        "def train_epoch(model, loader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    for data, labels in loader:\n",
        "        data, labels = data.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(data)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * data.size(0)\n",
        "\n",
        "        # Store predictions and true labels for metrics\n",
        "        all_preds.append(outputs.detach().cpu())\n",
        "        all_labels.append(labels.cpu())\n",
        "\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    all_preds_binary = (all_preds > 0.5).int()\n",
        "    accuracy = accuracy_score(all_labels.numpy(), all_preds_binary.numpy())\n",
        "    precision = precision_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "\n",
        "    return running_loss / len(loader.dataset), accuracy, precision, recall\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HU8YJAKIT3gV"
      },
      "outputs": [],
      "source": [
        "\n",
        "def validate_epoch(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss = 0.0\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data, labels in loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            outputs = model(data)\n",
        "            loss = criterion(outputs, labels)\n",
        "            running_loss += loss.item() * data.size(0)\n",
        "\n",
        "            # Store predictions and true labels for metric calculation\n",
        "            all_preds.append(outputs.cpu())\n",
        "            all_labels.append(labels.cpu())\n",
        "    all_preds = torch.cat(all_preds)\n",
        "    all_labels = torch.cat(all_labels)\n",
        "\n",
        "    # Compute metrics\n",
        "    all_preds_binary = (all_preds > 0.5).int()\n",
        "    accuracy = accuracy_score(all_labels.numpy(), all_preds_binary.numpy())\n",
        "    precision = precision_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "    recall = recall_score(all_labels.numpy(), all_preds_binary.numpy(), average=\"macro\", zero_division=0)\n",
        "\n",
        "    return running_loss / len(loader.dataset), accuracy, precision, recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hUByb0yET7BO",
        "outputId": "919b5ba7-93e9-4b71-f561-6af935849a35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "Train Loss: 0.5491, Accuracy: 0.3779, Precision: 0.3553, Recall: 0.3401\n",
            "Validation Loss: 0.6341, Accuracy: 0.1583, Precision: 0.4306, Recall: 0.3590\n",
            "Epoch 2/20\n",
            "Train Loss: 0.4359, Accuracy: 0.4292, Precision: 0.6008, Recall: 0.4415\n",
            "Validation Loss: 0.6245, Accuracy: 0.2250, Precision: 0.4870, Recall: 0.4644\n",
            "Epoch 3/20\n",
            "Train Loss: 0.3037, Accuracy: 0.6389, Precision: 0.8191, Recall: 0.6945\n",
            "Validation Loss: 0.6581, Accuracy: 0.2583, Precision: 0.7466, Recall: 0.6712\n",
            "Epoch 4/20\n",
            "Train Loss: 0.1930, Accuracy: 0.7823, Precision: 0.8844, Recall: 0.8451\n",
            "Validation Loss: 0.6194, Accuracy: 0.3500, Precision: 0.7580, Recall: 0.6391\n",
            "Epoch 5/20\n",
            "Train Loss: 0.1308, Accuracy: 0.8425, Precision: 0.9147, Recall: 0.9164\n",
            "Validation Loss: 0.8839, Accuracy: 0.3667, Precision: 0.7480, Recall: 0.7464\n",
            "Epoch 6/20\n",
            "Train Loss: 0.0969, Accuracy: 0.9018, Precision: 0.9525, Recall: 0.9313\n",
            "Validation Loss: 0.7643, Accuracy: 0.3250, Precision: 0.7335, Recall: 0.6751\n",
            "Epoch 7/20\n",
            "Train Loss: 0.0646, Accuracy: 0.9345, Precision: 0.9712, Recall: 0.9538\n",
            "Validation Loss: 1.0538, Accuracy: 0.3667, Precision: 0.7835, Recall: 0.6024\n",
            "Epoch 8/20\n",
            "Train Loss: 0.0437, Accuracy: 0.9611, Precision: 0.9819, Recall: 0.9744\n",
            "Validation Loss: 0.9267, Accuracy: 0.3917, Precision: 0.7149, Recall: 0.8131\n",
            "Epoch 9/20\n",
            "Train Loss: 0.0407, Accuracy: 0.9637, Precision: 0.9844, Recall: 0.9788\n",
            "Validation Loss: 1.2459, Accuracy: 0.4000, Precision: 0.7537, Recall: 0.7687\n",
            "Epoch 10/20\n",
            "Train Loss: 0.0380, Accuracy: 0.9602, Precision: 0.9825, Recall: 0.9789\n",
            "Validation Loss: 0.9063, Accuracy: 0.4333, Precision: 0.7704, Recall: 0.7913\n",
            "Epoch 11/20\n",
            "Train Loss: 0.0589, Accuracy: 0.9354, Precision: 0.9713, Recall: 0.9616\n",
            "Validation Loss: 0.9694, Accuracy: 0.3250, Precision: 0.7605, Recall: 0.6847\n",
            "Epoch 12/20\n",
            "Train Loss: 0.0324, Accuracy: 0.9619, Precision: 0.9843, Recall: 0.9723\n",
            "Validation Loss: 1.1261, Accuracy: 0.3500, Precision: 0.7513, Recall: 0.6634\n",
            "Epoch 13/20\n",
            "Train Loss: 0.0238, Accuracy: 0.9779, Precision: 0.9893, Recall: 0.9870\n",
            "Validation Loss: 0.9199, Accuracy: 0.3917, Precision: 0.7459, Recall: 0.7699\n",
            "Epoch 14/20\n",
            "Train Loss: 0.0139, Accuracy: 0.9841, Precision: 0.9942, Recall: 0.9923\n",
            "Validation Loss: 1.0984, Accuracy: 0.3167, Precision: 0.7281, Recall: 0.7767\n",
            "Epoch 15/20\n",
            "Train Loss: 0.0221, Accuracy: 0.9788, Precision: 0.9904, Recall: 0.9896\n",
            "Validation Loss: 1.0497, Accuracy: 0.3583, Precision: 0.7866, Recall: 0.6636\n",
            "Epoch 16/20\n",
            "Train Loss: 0.0270, Accuracy: 0.9743, Precision: 0.9863, Recall: 0.9845\n",
            "Validation Loss: 1.0722, Accuracy: 0.3250, Precision: 0.7574, Recall: 0.7507\n",
            "Epoch 17/20\n",
            "Train Loss: 0.0246, Accuracy: 0.9743, Precision: 0.9858, Recall: 0.9859\n",
            "Validation Loss: 1.1319, Accuracy: 0.3833, Precision: 0.7845, Recall: 0.6420\n",
            "Epoch 18/20\n",
            "Train Loss: 0.0108, Accuracy: 0.9903, Precision: 0.9975, Recall: 0.9915\n",
            "Validation Loss: 1.1616, Accuracy: 0.3333, Precision: 0.7454, Recall: 0.7514\n",
            "Epoch 19/20\n",
            "Train Loss: 0.0099, Accuracy: 0.9929, Precision: 0.9944, Recall: 0.9964\n",
            "Validation Loss: 1.2466, Accuracy: 0.3000, Precision: 0.7410, Recall: 0.6615\n",
            "Epoch 20/20\n",
            "Train Loss: 0.0142, Accuracy: 0.9903, Precision: 0.9939, Recall: 0.9927\n",
            "Validation Loss: 1.1957, Accuracy: 0.3250, Precision: 0.7485, Recall: 0.6604\n"
          ]
        }
      ],
      "source": [
        "num_epochs = 20\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    train_loss, train_accuracy, train_precision, train_recall = train_epoch(\n",
        "        model, train_loader, criterion, optimizer, device\n",
        "    )\n",
        "\n",
        "    # Validation\n",
        "    val_loss, val_accuracy, val_precision, val_recall = validate_epoch(\n",
        "        model, valid_loader, criterion, device\n",
        "    )\n",
        "\n",
        "    # Print metrics\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.4f}, Precision: {train_precision:.4f}, Recall: {train_recall:.4f}\")\n",
        "    print(f\"Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.4f}, Precision: {val_precision:.4f}, Recall: {val_recall:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation of Results of ResNet 200d:\n",
        "\n",
        "### **Epochs 1-5**\n",
        "\n",
        "| Epoch | Train Loss | Train Accuracy | Train Precision | Train Recall | Validation Loss | Validation Accuracy | Validation Precision | Validation Recall |\n",
        "|-------|------------|----------------|------------------|--------------|-----------------|---------------------|----------------------|--------------------|\n",
        "| 1     | 0.5491     | 0.3779         | 0.3553           | 0.3401       | 0.6341          | 0.1583              | 0.4306               | 0.3590             |\n",
        "| 2     | 0.4359     | 0.4292         | 0.6008           | 0.4415       | 0.6245          | 0.2250              | 0.4870               | 0.4644             |\n",
        "| 3     | 0.3037     | 0.6389         | 0.8191           | 0.6945       | 0.6581          | 0.2583              | 0.7466               | 0.6712             |\n",
        "| 4     | 0.1930     | 0.7823         | 0.8844           | 0.8451       | 0.6194          | 0.3500              | 0.7580               | 0.6391             |\n",
        "| 5     | 0.1308     | 0.8425         | 0.9147           | 0.9164       | 0.8839          | 0.3667              | 0.7480               | 0.7464             |\n",
        "\n",
        "**Observations:**\n",
        "- **Training Metrics:** There's a significant improvement in training loss and accuracy, with precision and recall steadily increasing. By Epoch 5, the model achieves high accuracy (~84%) and strong precision and recall scores (~91% each), indicating effective learning on the training data.\n",
        "- **Validation Metrics:** Despite improvements in training, validation loss fluctuates and even increases by Epoch 5. Validation accuracy remains substantially lower (~36%) compared to training, and precision and recall are inconsistent. This discrepancy suggests that the model is starting to **overfit** the training data, capturing noise rather than generalizable patterns.\n",
        "\n",
        "---\n",
        "\n",
        "### **Epochs 6-10**\n",
        "\n",
        "| Epoch | Train Loss | Train Accuracy | Train Precision | Train Recall | Validation Loss | Validation Accuracy | Validation Precision | Validation Recall |\n",
        "|-------|------------|----------------|------------------|--------------|-----------------|---------------------|----------------------|--------------------|\n",
        "| 6     | 0.0969     | 0.9018         | 0.9525           | 0.9313       | 0.7643          | 0.3250              | 0.7335               | 0.6751             |\n",
        "| 7     | 0.0646     | 0.9345         | 0.9712           | 0.9538       | 1.0538          | 0.3667              | 0.7835               | 0.6024             |\n",
        "| 8     | 0.0437     | 0.9611         | 0.9819           | 0.9744       | 0.9267          | 0.3917              | 0.7149               | 0.8131             |\n",
        "| 9     | 0.0407     | 0.9637         | 0.9844           | 0.9788       | 1.2459          | 0.4000              | 0.7537               | 0.7687             |\n",
        "| 10    | 0.0380     | 0.9602         | 0.9825           | 0.9789       | 0.9063          | 0.4333              | 0.7704               | 0.7913             |\n",
        "\n",
        "**Observations:**\n",
        "- **Training Metrics:** Training continues to excel, with loss dropping below 0.05 and accuracy exceeding 96%. Precision and recall remain exceptionally high (~98% and ~97%, respectively), reinforcing the model's strong fit to the training data.\n",
        "- **Validation Metrics:** Validation loss spikes significantly in Epochs 7 and 9, while validation accuracy shows marginal improvements, peaking around ~43%. Precision and recall on validation data fluctuate without consistent gains. The widening gap between training and validation performance underscores ongoing **overfitting**, where the model's ability to generalize to unseen data is compromised.\n",
        "\n",
        "---\n",
        "\n",
        "### **Epochs 11-15**\n",
        "\n",
        "| Epoch | Train Loss | Train Accuracy | Train Precision | Train Recall | Validation Loss | Validation Accuracy | Validation Precision | Validation Recall |\n",
        "|-------|------------|----------------|------------------|--------------|-----------------|---------------------|----------------------|--------------------|\n",
        "| 11    | 0.0589     | 0.9354         | 0.9713           | 0.9616       | 0.9694          | 0.3250              | 0.7605               | 0.6847             |\n",
        "| 12    | 0.0324     | 0.9619         | 0.9843           | 0.9723       | 1.1261          | 0.3500              | 0.7513               | 0.6634             |\n",
        "| 13    | 0.0238     | 0.9779         | 0.9893           | 0.9870       | 0.9199          | 0.3917              | 0.7459               | 0.7699             |\n",
        "| 14    | 0.0139     | 0.9841         | 0.9942           | 0.9923       | 1.0984          | 0.3167              | 0.7281               | 0.7767             |\n",
        "| 15    | 0.0221     | 0.9788         | 0.9904           | 0.9896       | 1.0497          | 0.3583              | 0.7866               | 0.6636             |\n",
        "\n",
        "**Observations:**\n",
        "- **Training Metrics:** The model continues to improve, reaching up to ~98% accuracy and maintaining near-perfect precision and recall (~99% each). This indicates an even deeper **overfitting**, as the model becomes increasingly specialized to the training data.\n",
        "- **Validation Metrics:** Validation loss remains high and volatile, with minor fluctuations in accuracy. Precision and recall show some improvement but remain inconsistent. The persistent high validation loss alongside soaring training performance further confirms that the model is **overfitting** and struggling to generalize.\n",
        "\n",
        "---\n",
        "\n",
        "### **Epochs 16-20**\n",
        "\n",
        "| Epoch | Train Loss | Train Accuracy | Train Precision | Train Recall | Validation Loss | Validation Accuracy | Validation Precision | Validation Recall |\n",
        "|-------|------------|----------------|------------------|--------------|-----------------|---------------------|----------------------|--------------------|\n",
        "| 16    | 0.0270     | 0.9743         | 0.9863           | 0.9845       | 1.0722          | 0.3250              | 0.7574               | 0.7507             |\n",
        "| 17    | 0.0246     | 0.9743         | 0.9858           | 0.9859       | 1.1319          | 0.3833              | 0.7845               | 0.6420             |\n",
        "| 18    | 0.0108     | 0.9903         | 0.9975           | 0.9915       | 1.1616          | 0.3333              | 0.7454               | 0.7514             |\n",
        "| 19    | 0.0099     | 0.9929         | 0.9944           | 0.9964       | 1.2466          | 0.3000              | 0.7410               | 0.6615             |\n",
        "| 20    | 0.0142     | 0.9903         | 0.9939           | 0.9927       | 1.1957          | 0.3250              | 0.7485               | 0.6604             |\n",
        "\n",
        "**Observations:**\n",
        "- **Training Metrics:** By Epoch 20, training loss plummets to ~0.0099, with accuracy soaring to ~99%. Precision and recall remain extraordinarily high (~99% each). This extreme performance on training data is a clear sign of **overfitting**, as the model has learned the training data almost perfectly.\n",
        "- **Validation Metrics:** Validation loss continues to increase, and validation accuracy shows no consistent improvement, hovering around ~30-38%. Precision and recall on validation data remain low and unstable. The stark contrast between training and validation performance solidifies the observation that the model is **overfitting** the training dataset.\n",
        "\n",
        "---\n",
        "\n",
        "### **Summary of Metrics**\n",
        "\n",
        "- **Training Performance:**\n",
        "  - **Loss:** Decreases consistently from ~0.55 to ~0.0099.\n",
        "  - **Accuracy:** Rises sharply from ~38% to ~99%.\n",
        "  - **Precision & Recall:** Both metrics soar, indicating that the model is making highly accurate and comprehensive predictions on the training data.\n",
        "\n",
        "- **Validation Performance:**\n",
        "  - **Loss:** Generally increases from ~0.63 to ~1.25, with minor fluctuations.\n",
        "  - **Accuracy:** Remains low (~15-43%) and does not follow the training trend.\n",
        "  - **Precision & Recall:** Fluctuate without significant improvement, indicating inconsistent performance on unseen data.\n",
        "\n",
        "**Conclusion:** The ResNet-200d model exhibits clear **overfitting**. While it performs exceptionally well on the training data, its ability to generalize to validation data is poor, as evidenced by the increasing validation loss and stagnant validation accuracy.\n",
        "\n",
        "**Next Steps:** To address the overfitting issue, we will now try a **3D ResNet**. This architecture is better suited for volumetric data like MRI scans, potentially enhancing the model's ability to capture spatial hierarchies and improving generalization performance."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
