{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_t8lcxrv0CU",
        "outputId": "f6ce1a16-b66d-42ce-da9b-e68b08b2e229"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from scipy.ndimage import zoom\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.animation import FuncAnimation, FFMpegWriter\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import warnings\n",
        "from sklearn.exceptions import UndefinedMetricWarning\n",
        "from sklearn.metrics import roc_curve, auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, log_loss\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.models.video import r3d_18"
      ],
      "metadata": {
        "id": "YVVjF_stwJ5D"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "Ynyr1RTDSlzB"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = \"/content/drive/MyDrive/MRNet-v1.0\"\n",
        "train_path = os.path.join(main_dir, \"train\")\n",
        "valid_path = os.path.join(main_dir, \"valid\")"
      ],
      "metadata": {
        "id": "OuIPGV5gwN6F"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/MRNet-v1.0/train\"\n",
        "file_id = \"0000\"\n",
        "\n",
        "axial_path = f\"{base_dir}/axial/{file_id}.npy\"\n",
        "coronal_path = f\"{base_dir}/coronal/{file_id}.npy\"\n",
        "sagittal_path = f\"{base_dir}/sagittal/{file_id}.npy\"\n",
        "\n",
        "# Load MRI scans from the .npy files\n",
        "axial_scan = np.load(axial_path)\n",
        "coronal_scan = np.load(coronal_path)\n",
        "sagittal_scan = np.load(sagittal_path)\n",
        "\n",
        "# Check the shape of each MRI scan\n",
        "print(\"Axial scan shape:\", axial_scan.shape)\n",
        "print(\"Coronal scan shape:\", coronal_scan.shape)\n",
        "print(\"Sagittal scan shape:\", sagittal_scan.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFz1mMT0wOkv",
        "outputId": "46d369b5-bd55-44af-b62d-2ad9d877cad3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Axial scan shape: (44, 256, 256)\n",
            "Coronal scan shape: (36, 256, 256)\n",
            "Sagittal scan shape: (36, 256, 256)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load labels from CSV files\n",
        "train_abnormal = pd.read_csv(os.path.join(main_dir, \"train-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "train_acl = pd.read_csv(os.path.join(main_dir, \"train-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "train_meniscus = pd.read_csv(os.path.join(main_dir, \"train-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "\n",
        "valid_abnormal = pd.read_csv(os.path.join(main_dir, \"valid-abnormal.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "valid_acl = pd.read_csv(os.path.join(main_dir, \"valid-acl.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n",
        "valid_meniscus = pd.read_csv(os.path.join(main_dir, \"valid-meniscus.csv\"), header=None, index_col=0).squeeze(\"columns\").to_dict()\n"
      ],
      "metadata": {
        "id": "HHj1bFQZP3xj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to resize the depth of a scan to a target depth\n",
        "def resize_depth(scan, target_depth):\n",
        "    depth_factor = target_depth / scan.shape[0]\n",
        "    return zoom(scan, (depth_factor, 1, 1), order=1)"
      ],
      "metadata": {
        "id": "DJQbFwJCP416"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to pad a scan to a target shape\n",
        "def pad_to_shape(scan, target_shape):\n",
        "    padded_scan = np.zeros(target_shape, dtype=scan.dtype)\n",
        "    min_d, min_h, min_w = min(scan.shape[0], target_shape[0]), min(scan.shape[1], target_shape[1]), min(scan.shape[2], target_shape[2])\n",
        "    padded_scan[:min_d, :min_h, :min_w] = scan[:min_d, :min_h, :min_w]\n",
        "    return padded_scan"
      ],
      "metadata": {
        "id": "Ktlwb5XCP77A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load a specific range of MRI data with labels\n",
        "\n",
        "def load_mri_data(data_type=\"train\", start_idx=0, end_idx=9, target_shape=(48, 256, 256), target_depth=48):\n",
        "    \"\"\"\n",
        "    Loads MRI data from a specified range and resizes/pads each scan to a target shape.\n",
        "    Parameters:\n",
        "    - data_type: \"train\" or \"valid\"\n",
        "    - start_idx, end_idx: Range of file indices to load (e.g., 0 to 9 for train, 1130 to 1249 for valid)\n",
        "    - target_shape: Target shape for each scan after resizing and padding\n",
        "    - target_depth: Target depth for each scan to ensure consistent depth\n",
        "    \"\"\"\n",
        "    # Set data path and range\n",
        "    data_path = train_path if data_type == \"train\" else valid_path\n",
        "    axial_path, coronal_path, sagittal_path = Path(data_path) / \"axial\", Path(data_path) / \"coronal\", Path(data_path) / \"sagittal\"\n",
        "\n",
        "    # Select the appropriate labels dictionary based on data type\n",
        "    abnormal_labels = train_abnormal if data_type == \"train\" else valid_abnormal\n",
        "    acl_labels = train_acl if data_type == \"train\" else valid_acl\n",
        "    meniscus_labels = train_meniscus if data_type == \"train\" else valid_meniscus\n",
        "\n",
        "    # Initialize lists to store data and labels\n",
        "    mri_data, labels = [], []\n",
        "\n",
        "    # Load each MRI scan within the specified range\n",
        "    for i in range(start_idx, end_idx + 1):\n",
        "        # Generate file name with zero-padded format (e.g., 0000, 0001, ...)\n",
        "        file_name = f\"{i:04}.npy\"\n",
        "\n",
        "        # Load and process each view with resizing and padding\n",
        "        axial_scan = pad_to_shape(resize_depth(np.load(axial_path / file_name), target_depth), target_shape)\n",
        "        coronal_scan = pad_to_shape(resize_depth(np.load(coronal_path / file_name), target_depth), target_shape)\n",
        "        sagittal_scan = pad_to_shape(resize_depth(np.load(sagittal_path / file_name), target_depth), target_shape)\n",
        "\n",
        "        # Combine the three views into one structure (3, depth, height, width)\n",
        "        combined_scan = np.stack([axial_scan, coronal_scan, sagittal_scan], axis=0)\n",
        "        mri_data.append(combined_scan)\n",
        "\n",
        "        # Retrieve actual labels for the current scan\n",
        "        abnormal_label = abnormal_labels.get(i, 0)  # Default to 0 if label is missing\n",
        "        acl_label = acl_labels.get(i, 0)\n",
        "        meniscus_label = meniscus_labels.get(i, 0)\n",
        "\n",
        "        # Append the actual labels\n",
        "        labels.append({\"abnormal\": abnormal_label, \"acl\": acl_label, \"meniscus\": meniscus_label})\n",
        "\n",
        "    return np.array(mri_data), labels"
      ],
      "metadata": {
        "id": "AizLBsHyP9sG"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the parameters for loading a smaller dataset\n",
        "# start_indices = list(range(0, 400, 100))  # Load only the first 400 samples in smaller batches\n",
        "# end_indices = [min(start + 99, 399) for start in start_indices]  # Ensure the last batch ends at 399\n",
        "\n",
        "# # Initialize lists to store all data and labels\n",
        "# all_data, all_labels = [], []\n",
        "\n",
        "# # Sequentially load data without using ThreadPoolExecutor\n",
        "# for start, end in zip(start_indices, end_indices):\n",
        "#     print(f\"Loading data for indices: {start} to {end}...\")\n",
        "#     data, labels = load_mri_data(data_type=\"train\", start_idx=start, end_idx=end)  # Load data batch\n",
        "#     all_data.append(data)\n",
        "#     all_labels.extend(labels)  # Extend to add lists of labels directly\n",
        "#     print(f\"Loaded batch: {start}-{end}\")\n",
        "\n",
        "# # Concatenate all data batches into a single array\n",
        "# train_data = np.concatenate(all_data, axis=0)\n",
        "# train_labels = all_labels  # Already extended to combine all label lists\n",
        "\n",
        "# # Check the final shape of training data and labels\n",
        "# print(\"Final training data shape:\", train_data.shape)  # Example: (400, 3, 48, 256, 256)\n",
        "# print(\"Final number of training labels:\", len(train_labels))  # Should match the number of samples in train_data\n"
      ],
      "metadata": {
        "id": "itE1GsbzP_ja"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the parameters for loading a very small dataset\n",
        "start_indices = [0]  # Start index for the small subset\n",
        "end_indices = [19]  # Load only the first 20 samples (end index is inclusive)\n",
        "\n",
        "# Initialize lists to store all data and labels\n",
        "all_data, all_labels = [], []\n",
        "\n",
        "# Sequentially load the small dataset\n",
        "for start, end in zip(start_indices, end_indices):\n",
        "    print(f\"Loading data for indices: {start} to {end}...\")\n",
        "    data, labels = load_mri_data(data_type=\"train\", start_idx=start, end_idx=end)  # Load data batch\n",
        "    all_data.append(data)\n",
        "    all_labels.extend(labels)  # Extend to add lists of labels directly\n",
        "    print(f\"Loaded batch: {start}-{end}\")\n",
        "\n",
        "# Concatenate all data batches into a single array\n",
        "train_data = np.concatenate(all_data, axis=0)\n",
        "train_labels = all_labels  # Already extended to combine all label lists\n",
        "\n",
        "# Check the final shape of training data and labels\n",
        "print(\"Final training data shape:\", train_data.shape)  # Example: (20, 3, 48, 256, 256)\n",
        "print(\"Final number of training labels:\", len(train_labels))  # Should match the number of samples in train_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxr6VS-mrCtQ",
        "outputId": "4a1d6f93-7a6d-4753-b359-96d2186e9d93"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data for indices: 0 to 19...\n",
            "Loaded batch: 0-19\n",
            "Final training data shape: (20, 3, 48, 256, 256)\n",
            "Final number of training labels: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the validation data from indices 1130 to 1249\n",
        "# valid_data, valid_labels = load_mri_data(data_type=\"valid\", start_idx=1130, end_idx=1249)\n",
        "# print(\"Validation data shape:\", valid_data.shape)  # Expected: (120, 3, 48, 256, 256)\n",
        "\n",
        "# Load only a small subset of validation data (20 samples)\n",
        "valid_data, valid_labels = load_mri_data(data_type=\"valid\", start_idx=1130, end_idx=1149)\n",
        "\n",
        "# Check the shape of validation data and labels\n",
        "print(\"Validation data shape:\", valid_data.shape)  # Example: (20, 3, 48, 256, 256)\n",
        "print(\"Number of validation labels:\", len(valid_labels))  # Should match the number of samples in valid_data\n"
      ],
      "metadata": {
        "id": "4k_mJ5VfQBe3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e3ba87a-ccec-4873-f132-225a7b7e9f9d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation data shape: (20, 3, 48, 256, 256)\n",
            "Number of validation labels: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set display options to avoid truncation\n",
        "pd.set_option('display.max_rows', None)      # Show all rows in the DataFrame\n",
        "pd.set_option('display.max_columns', None)   # Show all columns in the DataFrame\n",
        "pd.set_option('display.width', None)         # Expand display width to accommodate more columns\n",
        "pd.set_option('display.max_colwidth', None)  # Expand column width if necessary"
      ],
      "metadata": {
        "id": "g5qNv2c7QEOT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Flexible 3D ResNet model class\n",
        "class ResNet3D(nn.Module):\n",
        "    def __init__(self, pretrained=True, num_classes=3, optimizer_type=\"adam\"):\n",
        "        super(ResNet3D, self).__init__()\n",
        "        self.optimizer_type = optimizer_type  # Store optimizer type as part of the model\n",
        "\n",
        "        # Load a pre-trained 3D ResNet-18 model\n",
        "        self.resnet3d = r3d_18(pretrained=pretrained)\n",
        "\n",
        "        # Replace the final fully connected layer to match the desired output size\n",
        "        in_features = self.resnet3d.fc.in_features\n",
        "        self.resnet3d.fc = nn.Linear(in_features, num_classes)  # 3 binary outputs (one per class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.resnet3d(x)\n",
        "\n",
        "# Define the loss function with BCEWithLogitsLoss\n",
        "def calculate_loss(preds, targets):\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    return criterion(preds, targets.float())\n",
        "\n",
        "# Compute metrics for multi-label classification\n",
        "def compute_metrics(y_true, y_pred, y_proba):\n",
        "    labels = [\"abnormal\", \"acl\", \"meniscus\"]  # Label names for multi-label classification\n",
        "    y_true_np, y_pred_np, y_proba_np = np.array(y_true), np.array(y_pred), np.array(y_proba)\n",
        "    results = {}\n",
        "    for i, label in enumerate(labels):\n",
        "        cm = confusion_matrix(y_true_np[:, i], y_pred_np[:, i], labels=[0, 1])\n",
        "        accuracy = accuracy_score(y_true_np[:, i], y_pred_np[:, i])\n",
        "        precision = precision_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
        "        recall = recall_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
        "        f1 = f1_score(y_true_np[:, i], y_pred_np[:, i], zero_division=0)\n",
        "        try:\n",
        "            log_loss_value = log_loss(y_true_np[:, i], y_proba_np[:, i], labels=[0, 1])\n",
        "        except ValueError:\n",
        "            log_loss_value = None\n",
        "\n",
        "        print(f\"\\nMetrics for {label}:\")\n",
        "        print(f\"Confusion Matrix:\\n{cm}\")\n",
        "        print(f\"Accuracy: {accuracy}\")\n",
        "        print(f\"Precision: {precision}\")\n",
        "        print(f\"Recall: {recall}\")\n",
        "        print(f\"F1-Score: {f1}\")\n",
        "        print(f\"Log Loss: {log_loss_value}\")\n",
        "\n",
        "        results[label] = {\n",
        "            'accuracy': accuracy,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1': f1,\n",
        "            'log_loss': log_loss_value\n",
        "        }\n",
        "    return results\n",
        "\n",
        "# Flatten metrics for logging\n",
        "def flatten_metrics(metrics, prefix):\n",
        "    flattened = {}\n",
        "    for label, label_metrics in metrics.items():\n",
        "        for metric_name, value in label_metrics.items():\n",
        "            flattened[f\"{prefix}_{label}_{metric_name}\"] = float(value) if isinstance(value, np.float64) else value\n",
        "    return flattened\n",
        "\n",
        "# Training and evaluation function\n",
        "def train_and_evaluate(model, train_loader, valid_loader, epochs=5, learning_rate=0.001):\n",
        "    if model.optimizer_type.lower() == \"adam\":\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    elif model.optimizer_type.lower() == \"sgd\":\n",
        "        optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer type. Choose 'adam' or 'sgd'.\")\n",
        "\n",
        "    results = []  # Store summarized epoch metrics for table output\n",
        "    detailed_metrics = []  # Store metrics for `detailed_df` with all per-class data\n",
        "    final_y_true_valid, final_y_pred_proba_valid = None, None  # To store final epoch validation values\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        y_true_train, y_pred_train, y_pred_proba_train = [], [], []\n",
        "\n",
        "        # Training loop\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = calculate_loss(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            y_true_train.extend(labels.numpy())\n",
        "            y_pred_train.extend((torch.sigmoid(outputs) > 0.5).detach().numpy())\n",
        "            y_pred_proba_train.extend(torch.sigmoid(outputs).detach().numpy())\n",
        "\n",
        "        # Compute training metrics\n",
        "        train_metrics = compute_metrics(np.array(y_true_train), np.array(y_pred_train), np.array(y_pred_proba_train))\n",
        "\n",
        "        model.eval()\n",
        "        valid_loss = 0\n",
        "        y_true_valid, y_pred_proba_valid = [], []\n",
        "\n",
        "        # Validation loop\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in valid_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = calculate_loss(outputs, labels)\n",
        "                valid_loss += loss.item()\n",
        "\n",
        "                y_true_valid.extend(labels.numpy())\n",
        "                y_pred_proba_valid.extend(torch.sigmoid(outputs).detach().numpy())\n",
        "\n",
        "        y_pred_valid = (np.array(y_pred_proba_valid) > 0.5).astype(int)\n",
        "        valid_metrics = compute_metrics(np.array(y_true_valid), y_pred_valid, np.array(y_pred_proba_valid))\n",
        "\n",
        "        # Update final validation metrics for last epoch outputs\n",
        "        final_y_true_valid = np.array(y_true_valid)\n",
        "        final_y_pred_proba_valid = np.array(y_pred_proba_valid)\n",
        "\n",
        "        # Add detailed metrics (all classes, all epochs) for `detailed_df`\n",
        "        for phase, metrics in zip([\"Train\", \"Validation\"], [train_metrics, valid_metrics]):\n",
        "            for label, label_metrics in metrics.items():\n",
        "                detailed_metrics.append({\n",
        "                    \"Epoch\": epoch + 1,\n",
        "                    \"Phase\": phase,\n",
        "                    \"Class\": label,\n",
        "                    \"Accuracy\": label_metrics[\"accuracy\"],\n",
        "                    \"Precision\": label_metrics[\"precision\"],\n",
        "                    \"Recall\": label_metrics[\"recall\"],\n",
        "                    \"F1-Score\": label_metrics[\"f1\"],\n",
        "                    \"Log Loss\": label_metrics[\"log_loss\"]\n",
        "                })\n",
        "\n",
        "        # Aggregate metrics for `results` (summary output without per-class breakdown)\n",
        "        overall_train_metrics = flatten_metrics(train_metrics, \"Train\")\n",
        "        overall_valid_metrics = flatten_metrics(valid_metrics, \"Valid\")\n",
        "\n",
        "        row_data = {\n",
        "            \"Epoch\": epoch + 1,\n",
        "            \"Train Loss\": train_loss / len(train_loader),\n",
        "            \"Valid Loss\": valid_loss / len(valid_loader),\n",
        "        }\n",
        "        row_data.update(overall_train_metrics)\n",
        "        row_data.update(overall_valid_metrics)\n",
        "        results.append(row_data)\n",
        "\n",
        "    # Display summarized DataFrame (`summary_df`) with per-epoch metrics\n",
        "    summary_df = pd.DataFrame(results)\n",
        "    pd.set_option('display.max_rows', None)\n",
        "    display(summary_df.style.set_table_styles([{'selector': 'th', 'props': [('font-weight', 'bold')]}]))\n",
        "\n",
        "    # Create `detailed_df` with per-class metrics in columns, suitable for further analysis\n",
        "    detailed_df = pd.DataFrame(detailed_metrics)\n",
        "    return final_y_true_valid, final_y_pred_proba_valid\n",
        "\n",
        "# Convert labels from dictionaries to multi-label binary tensors\n",
        "def convert_labels_to_tensor(labels):\n",
        "    labels_tensor = torch.zeros(len(labels), 3)  # Three classes: abnormal, ACL, meniscus\n",
        "    for i, label_dict in enumerate(labels):\n",
        "        labels_tensor[i, 0] = label_dict['abnormal']\n",
        "        labels_tensor[i, 1] = label_dict['acl']\n",
        "        labels_tensor[i, 2] = label_dict['meniscus']\n",
        "    return labels_tensor\n"
      ],
      "metadata": {
        "id": "hmSgr-DtSrwk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a list of dictionaries to a tensor\n",
        "def convert_labels_to_tensor(label_dicts):\n",
        "    # Convert each dictionary's values into a list and create a tensor from it\n",
        "    labels_as_lists = [list(label.values()) for label in label_dicts]\n",
        "    return torch.tensor(labels_as_lists, dtype=torch.float32)\n",
        "\n",
        "# Apply this to train and validation labels\n",
        "train_labels_tensor = convert_labels_to_tensor(train_labels)\n",
        "valid_labels_tensor = convert_labels_to_tensor(valid_labels)\n",
        "\n",
        "# Create TensorDatasets for training and validation data\n",
        "train_tensor = TensorDataset(torch.tensor(train_data).float(), train_labels_tensor)\n",
        "valid_tensor = TensorDataset(torch.tensor(valid_data).float(), valid_labels_tensor)\n",
        "\n",
        "# Adjust batch size to avoid high memory usage\n",
        "batch_size = 16  # Adjust based on available memory\n",
        "\n",
        "# Create DataLoaders with updated parameters\n",
        "train_loader = DataLoader(\n",
        "    train_tensor,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=4  # Adjust based on your CPU cores and memory\n",
        ")\n",
        "\n",
        "valid_loader = DataLoader(\n",
        "    valid_tensor,\n",
        "    batch_size=batch_size,\n",
        "    pin_memory=True,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "# Print out DataLoader settings for confirmation\n",
        "print(f\"Train DataLoader - Batch Size: {batch_size}, Num Workers: 4\")\n",
        "print(f\"Validation DataLoader - Batch Size: {batch_size}, Num Workers: 4\")\n"
      ],
      "metadata": {
        "id": "XbmQYxR5QR5c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f316db15-4df7-4e30-c3d9-c3907f1c821d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train DataLoader - Batch Size: 16, Num Workers: 4\n",
            "Validation DataLoader - Batch Size: 16, Num Workers: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_roc_and_calculate_auc(y_true, y_pred_proba, class_names=[\"abnormal\", \"acl\", \"meniscus\"]):\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    auc_scores = {}\n",
        "\n",
        "    for i in range(y_true.shape[1]):  # Assuming y_true and y_pred_proba are (num_samples, num_classes)\n",
        "        fpr, tpr, _ = roc_curve(y_true[:, i], y_pred_proba[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        auc_scores[class_names[i]] = roc_auc  # Store AUC for each class\n",
        "\n",
        "        plt.plot(fpr, tpr, label=f\"{class_names[i]} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "    # Plot the random classifier line\n",
        "    plt.plot([0, 1], [0, 1], \"k--\")\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"ROC Curve for Each Class\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    # Print AUC scores for each class\n",
        "    print(\"AUC Scores for each class:\")\n",
        "    for class_name, auc_score in auc_scores.items():\n",
        "        print(f\"{class_name}: {auc_score:.2f}\")"
      ],
      "metadata": {
        "id": "vgKp7wZlQTxK"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = ResNet3D(pretrained=True, num_classes=3, optimizer_type=\"adam\")\n",
        "\n",
        "# Run the training and evaluation function and capture the outputs\n",
        "y_true_valid_np, y_pred_proba_valid_np = train_and_evaluate(model, train_loader, valid_loader, epochs=3, learning_rate=0.001)\n",
        "\n",
        "print(\"y_true_valid_np shape:\", y_true_valid_np.shape)\n",
        "print(\"y_pred_proba_valid_np shape:\", y_pred_proba_valid_np.shape)\n",
        "\n",
        "# Directly call the function with `y_true_valid_np` and `y_pred_proba_valid_np`\n",
        "plot_roc_and_calculate_auc(y_true_valid_np, y_pred_proba_valid_np)"
      ],
      "metadata": {
        "id": "srhCDbAYQWCs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bf73624-ab8d-47b5-8682-ce262c84c306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=R3D_18_Weights.KINETICS400_V1`. You can also use `weights=R3D_18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/r3d_18-b3b3357e.pth\" to /root/.cache/torch/hub/checkpoints/r3d_18-b3b3357e.pth\n",
            "100%|██████████| 127M/127M [00:00<00:00, 212MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Metrics for abnormal:\n",
            "Confusion Matrix:\n",
            "[[ 1  3]\n",
            " [ 0 16]]\n",
            "Accuracy: 0.85\n",
            "Precision: 0.8421052631578947\n",
            "Recall: 1.0\n",
            "F1-Score: 0.9142857142857143\n",
            "Log Loss: 0.5474359782750209\n",
            "\n",
            "Metrics for acl:\n",
            "Confusion Matrix:\n",
            "[[ 8 10]\n",
            " [ 2  0]]\n",
            "Accuracy: 0.4\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-Score: 0.0\n",
            "Log Loss: 0.6926997799983707\n",
            "\n",
            "Metrics for meniscus:\n",
            "Confusion Matrix:\n",
            "[[ 2 11]\n",
            " [ 0  7]]\n",
            "Accuracy: 0.45\n",
            "Precision: 0.3888888888888889\n",
            "Recall: 1.0\n",
            "F1-Score: 0.56\n",
            "Log Loss: 0.7440632135265691\n",
            "\n",
            "Metrics for abnormal:\n",
            "Confusion Matrix:\n",
            "[[20  0]\n",
            " [ 0  0]]\n",
            "Accuracy: 1.0\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-Score: 0.0\n",
            "Log Loss: 0.0241049041181986\n",
            "\n",
            "Metrics for acl:\n",
            "Confusion Matrix:\n",
            "[[20  0]\n",
            " [ 0  0]]\n",
            "Accuracy: 1.0\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-Score: 0.0\n",
            "Log Loss: 0.20722864624319337\n",
            "\n",
            "Metrics for meniscus:\n",
            "Confusion Matrix:\n",
            "[[20  0]\n",
            " [ 0  0]]\n",
            "Accuracy: 1.0\n",
            "Precision: 0.0\n",
            "Recall: 0.0\n",
            "F1-Score: 0.0\n",
            "Log Loss: 0.015220934326731403\n"
          ]
        }
      ]
    }
  ]
}